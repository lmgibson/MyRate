{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes to self:\n",
    "\n",
    "# The clustering on skills didn't work because there isn't enough skills per user.\n",
    "# Almost all users have about 5 skills, but there isn't enough overlap between them.\n",
    "# Futhermore, there is just about no signal relating the features to hourly rate.\n",
    "# Need to pivot and think of some alternative way to use this data.\n",
    "# Also, I need to figure out why everyone has a \"1\" for their invoices paid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Merging to get EDA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Packages for PostgreSQL Import and Export\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "# Packages for EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Packages for K-Modes Cluster\n",
    "import numpy as np\n",
    "from kmodes.kmodes import KModes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ideally I'll move this into the project config.py file\n",
    "# Otherwise for now I have to just manually assign . . .\n",
    "\n",
    "dbname = \"freelance_db\"\n",
    "username = \"Metaverse\"\n",
    "pswd = \"Arcifice91\"\n",
    "\n",
    "# Connect to Data\n",
    "con = None\n",
    "con = psycopg2.connect(database=dbname, user=username,\n",
    "                       host='localhost', password=pswd)\n",
    "\n",
    "# Checking shapes of tables\n",
    "sql_query = \"\"\"SELECT * from user_details_table;\"\"\"\n",
    "dtls_table = pd.read_sql_query(sql_query, con)\n",
    "\n",
    "sql_query = \"\"\"SELECT * from freelance_table;\"\"\"\n",
    "fl_table = pd.read_sql_query(sql_query, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merging\n",
    "# Modifying profile_url in the details table so I can merge it with the fl_table\n",
    "dtls_table['profile_url'] = \"https://www.guru.com\"+dtls_table['profile_url']\n",
    "\n",
    "# Now I'm merging them together to make dt\n",
    "dt = pd.merge(fl_table, dtls_table, on='profile_url')\n",
    "dt.shape\n",
    "dt = dt.drop(columns=(['index_x']))\n",
    "dt = dt.drop_duplicates(subset='profile_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merging in data to crosswalk states with region labels\n",
    "state_region_crswlk = pd.read_csv(\n",
    "    '~/Desktop/Insight/projects/myrate/data/raw/census-regions/us census bureau regions and divisions.csv')\n",
    "state_region_crswlk.head()\n",
    "\n",
    "dt = pd.merge(dt, state_region_crswlk,\n",
    "                     how='left', left_on='state', right_on='State')\n",
    "\n",
    "# Fixing region value for individuals living in Puerto Rico\n",
    "dt.loc[dt['state'] == \"Puerto Rico\", 'Region'] = \"Other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Outcome: Hourly Rate\n",
    "### Beginning with investigating the geographic labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean hourly rate ($): 31.94\n"
     ]
    }
   ],
   "source": [
    "# What is the overall mean?\n",
    "print(\"Mean hourly rate ($):\",round(dt['hourly_rate'].mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Region\n",
       "Midwest      29.724138\n",
       "Northeast    30.782609\n",
       "Other        20.000000\n",
       "South        31.972222\n",
       "West         34.197719\n",
       "Name: hourly_rate, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the mean by region?\n",
    "dt.groupby(['Region']).hourly_rate.mean()\n",
    "\n",
    "# There is some variation, but it is minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     50.000000\n",
       "mean      33.764035\n",
       "std       15.197060\n",
       "min        8.000000\n",
       "25%       26.650000\n",
       "50%       32.175057\n",
       "75%       36.732353\n",
       "max      104.000000\n",
       "Name: hourly_rate, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the mean across states?\n",
    "dt.groupby(['state']).hourly_rate.mean().sort_values().describe()\n",
    "\n",
    "# Pretty decent spread. This may be the most powerful signal yet.\n",
    "# If you print out hte full table it is clear that there is variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the variation within states?\n",
    "state_lvl_hourly_rate_descriptives = {}\n",
    "state_lvl_hourly_rate_descriptives['min'] = dt.groupby(['state']).hourly_rate.min()\n",
    "state_lvl_hourly_rate_descriptives['mean'] = dt.groupby(['state']).hourly_rate.mean()\n",
    "state_lvl_hourly_rate_descriptives['max'] = dt.groupby(['state']).hourly_rate.max()\n",
    "state_lvl_hourly_rate_descriptives['var'] = dt.groupby(['state']).hourly_rate.var()\n",
    "dt_state_hr = pd.DataFrame(state_lvl_hourly_rate_descriptives)\n",
    "\n",
    "# A decent amount of variation within the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I tried to break it down further to the city level\n",
    "# But the city data is really messy and not worth the time sink\n",
    "# To try and get it set up. The idea would be to merge on\n",
    "# rural vs urban and see if it can help me filter out a signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the relationship between a users \"rating\" and the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cleaing it up real quick\n",
    "dt.rating = dt.rating.str.replace('%', '')\n",
    "dt.rating = dt.rating.str.replace('NA', '')\n",
    "dt.rating = pd.to_numeric(dt.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values:  68\n"
     ]
    }
   ],
   "source": [
    "# Starting with Missing Values\n",
    "print(\"Number of missing values: \",dt.rating.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    911.000000\n",
       "mean      98.424040\n",
       "std       11.376479\n",
       "min        1.000000\n",
       "25%      100.000000\n",
       "50%      100.000000\n",
       "75%      100.000000\n",
       "max      100.000000\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the spread look like for the rest?\n",
    "dt.rating.describe()\n",
    "\n",
    "# Basically no variation. Okay, let's make a binary variable (has_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_rating\n",
       "0    34.529412\n",
       "1    31.743139\n",
       "Name: hourly_rate, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating \"has_rating\"\n",
    "dt['has_rating'] = None\n",
    "dt.loc[dt['rating'].isna(), 'has_rating'] = 0\n",
    "dt.loc[dt['rating'] > 0, 'has_rating'] = 1\n",
    "\n",
    "# Comparing outcome means by has_rating\n",
    "dt.groupby(['has_rating']).hourly_rate.mean()\n",
    "\n",
    "# Small difference, nothing substantial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the duration of membership and hourly rate\n",
    "\n",
    "1. Convert member start date to a date object\n",
    "2. Calculate years / months they've been a member (to today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_convert(member_since):\n",
    "    try:\n",
    "        tmp = datetime.strptime(member_since, '%b, %Y')\n",
    "        #tmp = tmp.strftime('%Y-%m')\n",
    "    except:\n",
    "        tmp = 'NaN'\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def years_active(date):\n",
    "    cur_year = datetime.now().year\n",
    "    try:\n",
    "        yrs_active = cur_year - date.year\n",
    "    except:\n",
    "        yrs_active = 'NaN'\n",
    "\n",
    "    return yrs_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def months_active(date):\n",
    "    cur_year = datetime.now().year\n",
    "    cur_month = datetime.now().month\n",
    "\n",
    "    try:\n",
    "        mnths_active = (cur_year - date.year)*12 + (cur_month - date.month)\n",
    "    except:\n",
    "        mnths_active = 'NaN'\n",
    "\n",
    "    return mnths_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting member_since to date\n",
    "dt.member_since = dt.member_since.str.strip()\n",
    "dt['start_date'] = dt.member_since.apply(date_convert)\n",
    "dt['years_active'] = dt.start_date.apply(years_active)\n",
    "dt['months_active'] = dt.start_date.apply(months_active)\n",
    "\n",
    "# Therea are 31 NAs. Just going to ignore them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What I want to understand is:\n",
    "# How does the hourly rate (average) change across years active?\n",
    "dt.groupby('years_active').hourly_rate.mean()\n",
    "\n",
    "# Nothing much. It ramps up if years active > 17. Building that binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['active_17up'] = (dt['years_active'] >= 17) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring distribution of years active\n",
    "sns.distplot(dt.years_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring distribution of months active\n",
    "sns.distplot(dt.months_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discussing distributions of years active\n",
    "# Pretty similar, heavy towards the low end with some people\n",
    "# who have been active for a very long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it compare with hourly rate? Not much there.\n",
    "sns.scatterplot(x='hourly_rate', y='years_active', data=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='hourly_rate', y='months_active', data=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about by region? I don't think there would be much here. Not worth the investment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving on to invoices rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the (obvious) relationship between memebershp time and # of jobs completed\n",
    "# How strong is the relationship?\n",
    "# First have to clean the invoices paid variable\n",
    "dt.invoices_paid = dt.invoices_paid.str.replace(',', '')\n",
    "dt.invoices_paid = pd.to_numeric(dt.invoices_paid)\n",
    "dt.invoices_paid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who is NA? I have a whole bunch of 1s but then some NAs?\n",
    "# The question for a poisson model is: did they just have no invoices OR\n",
    "# were they never going to have any invoices.\n",
    "# Looked at some of them and I'll treat them as zeros. So there are really that many ones?!?\n",
    "dt[dt.invoices_paid.isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning zero to the NAs. This is important!!\n",
    "dt.loc[dt.invoices_paid.isna(), 'invoices_paid'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dt.invoices_paid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow, very heavy on the zeros. Let's look closer.\n",
    "dt.invoices_paid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ah, so it is truncating at one.\n",
    "# This is going to be problematic for those who are actually at a value of one.\n",
    "# I can either rerun the scrape and create something to make the val = 0\n",
    "# or I can just move forward. . .\n",
    "\n",
    "# For now I'm going to keep moving forward and treat 1 as 0.\n",
    "# Note, it is more likely than not that they are = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dist vs months active (no 1s)\n",
    "sns.scatterplot(x='months_active', y='invoices_paid',\n",
    "                data=dt[dt.invoices_paid > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few massive outliers. Going to rough chop it down and replot\n",
    "invoice_gtr_one = dt.invoices_paid > 1\n",
    "invoice_ls_2000 = dt.invoices_paid < 2000\n",
    "sns.scatterplot(x='months_active', y='invoices_paid',\n",
    "                data=dt[invoice_gtr_one & invoice_ls_2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm, there may be a slight signal but you would expect a stronger trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dist vs years active (no 1s)\n",
    "sns.scatterplot(x='years_active', y='invoices_paid',\n",
    "                data=dt[invoice_gtr_one & invoice_ls_2000])\n",
    "# Slight postive non-linear trend bu there is a HUGE outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoices Paid - Notes\n",
    "\n",
    "It has a weak-ish relationship with time. There are a lot of really heavy users of the platform that dominate the market. Let's zoom into the first year and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at invoices pad in the first few months (among those only active for a shorter period of time)\n",
    "sns.scatterplot(x='months_active',\n",
    "                y='invoices_paid',\n",
    "                data=dt[dt['years_active'] < 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating invoices / time active\n",
    "dt['invoices_per_month'] = dt['invoices_paid'] / \\\n",
    "    dt['months_active']\n",
    "sns.distplot(dt.invoices_per_month)\n",
    "print(dt.invoices_per_month.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded on zero with a long tail. Continues to sell that there are some serious power users.\n",
    "# Average invoice per month is equal to one, but I left in the 1s that could be zeros. . .\n",
    "# Real average is likely near zero.\n",
    "\n",
    "# The question now is: Does this mean differ by any characteristic that I observe in the data?\n",
    "# For people with a different skill set (Andriod) do they have a higher mean invoice/month rate?\n",
    "# For people in different regions do they have a higher mean invoice/month rate?\n",
    "# For people with lower hourly rates do they have a higher mean invoice/month rate??\n",
    "\n",
    "# The next important step is to figure out how to use the skills data.\n",
    "# It would be great to try and create clusters of the skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Skills Data\n",
    "\n",
    "1. Obtain the dataset that is long by skills and shrink it down to just the skills. I'll work with that for this.\n",
    "2. Create some quick features such as: # of skills and top skill. These are user level so I can attach them back to dt to have everything in one place.\n",
    "\n",
    "Note: the skills are standardized on guru which makes this more straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"SELECT profile_url, skills_list from freelance_table;\"\"\"\n",
    "skills_table = pd.read_sql_query(sql_query, con)\n",
    "skills_table_dummies = pd.get_dummies(skills_table, columns=[\n",
    "                                      'skills_list'], prefix='').groupby(['profile_url']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of skills\n",
    "num_skills = skills_table.groupby('profile_url').count()\n",
    "num_skills.skills_list.describe()\n",
    "\n",
    "# Relatively uninformative because everyone maxes out the number of skilsl they can have.\n",
    "# However, I may be able to get some predictive power off the small group of people who have < 5 skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flagging those with < 5 skills\n",
    "num_skills['less_five_skills'] = (num_skills.skills_list < 5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting First Skill\n",
    "first_skill = skills_table[skills_table.groupby(\n",
    "    'profile_url')['skills_list'].cumcount() == 0]\n",
    "first_skill = first_skill.rename(\n",
    "    columns={\"profile_url\": \"profile_url\", \"skills_list\": \"first_skill\"})\n",
    "first_skill.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_skills = pd.merge(num_skills, first_skill, on='profile_url')\n",
    "num_skills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging into dt\n",
    "dt = pd.merge(dt, num_skills, on=\"profile_url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on clustering the skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found K-Modes on stackoverflow\n",
    "# Following the package documentation\n",
    "# Ref: https://pypi.org/project/kmodes/\n",
    "# Ref: https://stackoverflow.com/questions/42639824/python-k-modes-explanation\n",
    "# Ref: https://www.kaggle.com/ashydv/bank-customer-clustering-k-modes-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning column names from the dummies database\n",
    "skills_table_dummies.columns = skills_table_dummies.columns.str.strip().str.lower().str.replace(\n",
    "    ' ', '_').str.replace('(', '').str.replace(')', '').str.replace('_','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_table_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling with K Modes\n",
    "cost = []\n",
    "for num_clusters in list(range(20,30)):\n",
    "    kmode = KModes(n_clusters=num_clusters, init = \"Huang\", verbose=0)\n",
    "    kmode.fit_predict(skills_table_dummies)\n",
    "    cost.append(kmode.cost_)\n",
    "    print(\"Finished Cluster: \" + str(num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([i for i in range(20,30,1)])\n",
    "plt.plot(y,cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmode = KModes(n_clusters=28, init='Huang', verbose=0)\n",
    "clusters = kmode.fit_predict(skills_table_dummies)\n",
    "\n",
    "kmodes = kmode.cluster_centroids_\n",
    "shape = kmodes.shape\n",
    "\n",
    "for i in range(shape[0]):\n",
    "    if sum(kmodes[i,:]) == 0:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        print(\"no-skills cluster\")\n",
    "    else:\n",
    "        print(\"\\ncluster \" + str(i) + \": \")\n",
    "        cent = kmodes[i,:]\n",
    "        for j in skills_table_dummies.columns[np.nonzero(cent)]:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_assigned = km.predict(skills_table_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(clust_assigned, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cluster_crosswalk = pd.DataFrame(\n",
    "    skills_table['profile_url'].unique(), columns=[\"profile_url\"]).sort_values(by=\"profile_url\")\n",
    "user_cluster_crosswalk['cluster'] = clust_assigned\n",
    "user_cluster_crosswalk[user_cluster_crosswalk['cluster'] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging into the skills table to investigate those with \"no skills\"\n",
    "tmp = pd.merge(skills_table, user_cluster_crosswalk, on = \"profile_url\")\n",
    "tmp[tmp['cluster'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems to not be working great. \n",
    "# Why is it putting so many people into the no skills cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the process of using the \"bio\" data\n",
    "\n",
    "1. The easiest variable to create is the len(bio)\n",
    "2. I'll have to try and expand it more using some other techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bio(bio):\n",
    "    try:\n",
    "        cleaned_bio = ''.join(s for s in bio if ord(s) > 31 and ord(s) < 126)\n",
    "    except:\n",
    "        cleaned_bio = \"NaN\"\n",
    "    return cleaned_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_bio(bio):\n",
    "    if bio == \"NaN\":\n",
    "        return 0\n",
    "    else:\n",
    "        return len(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['bio_clean'] = dt.bio.apply(clean_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['bio_length'] = dt.bio_clean.apply(len_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the distrbution of bio lengths\n",
    "sns.distplot(dt.bio_length)\n",
    "\n",
    "# Nothing crazy, lots of 0s but a good number of people with bios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it relate to invoices / month?\n",
    "sns.scatterplot(x='bio_length',\n",
    "                y='invoices_per_month',\n",
    "                data=dt)\n",
    "\n",
    "# It's weak but I'll keep it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of separate words in bio\n",
    "dt['bio_word_count'] = dt['bio_clean'].apply(\n",
    "    lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length in bio\n",
    "def avg_word_ln(bio):\n",
    "    try:\n",
    "        words = bio.split()\n",
    "        res = (sum(len(word) for word in words)/len(words))\n",
    "    except:\n",
    "        res = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['avg_word_length'] = dt['bio_clean'].apply(avg_word_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stop words\n",
    "def num_stopwords(bio):\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    try:\n",
    "        res = len([x for x in bio.split() if x in stop])\n",
    "    except:\n",
    "        res = -1\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['num_stop'] = dt['bio_clean'].apply(num_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing before pulling more features\n",
    "\n",
    "# Remvoing stop words\n",
    "stop = stopwords.words('english')\n",
    "dt['bio_processed'] = dt['bio_clean'].apply(\n",
    "    lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Removing punctuation\n",
    "dt['bio_processed'] = dt['bio_processed'].str.replace(\n",
    "    '[^\\w\\s]', '')\n",
    "\n",
    "# Lower Case\n",
    "dt['bio_processed'] = dt['bio_processed'].apply(\n",
    "    lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some final cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Region into dummy variables\n",
    "region_dummies = pd.get_dummies(dt['Region'])\n",
    "dt = pd.concat([dt, region_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earnings per Month\n",
    "dt['earnings_ever'] = dt['earnings_ever'].str.replace('$', '')\n",
    "dt['earnings_ever'] = dt['earnings_ever'].str.replace(',', '')\n",
    "dt['earnings_ever'] = pd.to_numeric(dt['earnings_ever'])\n",
    "dt['earnings_pr_month'] = dt['earnings_ever'] / \\\n",
    "    dt['months_active']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving out Modified Dataset\n",
    "\n",
    "1. First printing the columns I have for reference.\n",
    "2. Saving it to a new table in the postgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning to just variables that will be used in the model\n",
    "analysis_dt = dt[['invoices_per_month', 'hourly_rate', 'earnings_pst_yr', 'earnings_pr_month',\n",
    "                         'Midwest', 'Northeast', 'South', 'Other', 'West', 'less_five_skills', 'bio_length',\n",
    "                         'bio_word_count', 'avg_word_length', 'num_stop', 'bio_processed']]\n",
    "analysis_dt = analysis_dt.dropna()  # Removing people with NA\n",
    "analysis_dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database and save data to it\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s' %\n",
    "                       (username, pswd, dbname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dt.to_sql(\"analysis_table\", engine, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
