{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details_scrape(url):\n",
    "    \"\"\"\n",
    "    Uses selenium to scrape additional information contained in a sub-menu\n",
    "    \n",
    "    Clicks the first box, guru throws up a log-in, goes back, opens all the boxes\n",
    "    and then finishes by extracting all the html from each box.\n",
    "    \n",
    "    This takes in a single URL (string) and extracts from that. One problem that may arise\n",
    "    is if it doesn't throw the log-in box and the driver tries to go back a page.\n",
    "    However, I think we can get around this by just loading each URL new. \n",
    "    \n",
    "    May be more efficient to page through? That's for another time.\n",
    "    \n",
    "    RETURNS: list of names and list of HTML for each details box.\n",
    "    \n",
    "    Test URL:\n",
    "    \"https://www.guru.com/d/freelancers/lc/united-states/california/los-angeles/pg/1/\"\n",
    "    \"\"\"\n",
    "    \n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "    first_url = driver.current_url\n",
    "    \n",
    "    # Checking if the site loaded\n",
    "    if len(login_form) == 0:\n",
    "        return None\n",
    "    \n",
    "    time.sleep(2) # Wait to click\n",
    "    login_form[2].click()\n",
    "\n",
    "    if driver.current_url != first_url:\n",
    "        time.sleep(2) # Wait for log-in prompt\n",
    "        driver.back() # Go back to main page\n",
    "        time.sleep(1) # Wait to scrape\n",
    "        login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "    else:\n",
    "        login_form[2].click()\n",
    "\n",
    "    counter = 2\n",
    "    \n",
    "    # There are multiple buttons so here I am just clicking the ones I want (2,6,10,...)\n",
    "    for i, val in enumerate(login_form):    \n",
    "        if i == counter:\n",
    "            login_form[i].click()\n",
    "            counter += 4\n",
    "    \n",
    "    # Pull elements in the now opened detail boxes\n",
    "        # //div[@class=\"avatarinfo\"]\n",
    "    user_name = driver.find_elements_by_xpath('//div[@class=\"module_avatar freelancerAvatar\"]')\n",
    "    user_detail = driver.find_elements_by_xpath('//div[@class=\"feedback\"]')\n",
    "\n",
    "    # Extract text from the names and HTML from the details\n",
    "    # Will parse the detail_html further with beautiful soup\n",
    "    # Each is a list of length equal to the number of users on the page\n",
    "    names = []\n",
    "    for i, val in enumerate(user_name):\n",
    "        names.append(val.get_attribute('innerHTML'))\n",
    "\n",
    "    detail_html = []\n",
    "    for i, val in enumerate(user_detail):\n",
    "        detail_html.append(val.get_attribute('innerHTML'))\n",
    "\n",
    "    driver.close()\n",
    "    \n",
    "    return names, detail_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_soup(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of raw htmls and parses them with BeautifulSoup.\n",
    "    \n",
    "    Returns a list of cleaner HTMLs (soup objects), \n",
    "    \n",
    "    \"\"\"\n",
    "    soups = []\n",
    "    for i, val in enumerate(x):\n",
    "        soups.append(BeautifulSoup(val,'html.parser'))\n",
    "        \n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_urls_to_list(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of soups and extracts user htmls from them.\n",
    "    \n",
    "    Returns a list with user htmls\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    user_htmls = []\n",
    "    for i, val in enumerate(x):\n",
    "        user_htmls.append(val.a['href'])\n",
    "        \n",
    "    return user_htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_details_to_list(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of soups and extracts the em from them.\n",
    "    \n",
    "    Returns a list with lists where the first element has a list containing the em elements.\n",
    "    The second element also has a list containing the em elements, and so on.\n",
    "    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for i, val in enumerate(x):\n",
    "        values.append(val.find_all('em'))\n",
    "        \n",
    "    html_to_string = lambda x: x.string\n",
    "    values_strings = []\n",
    "    for i, val in enumerate(values):\n",
    "        values_strings.append(list(map(html_to_string, val)))\n",
    "        \n",
    "    return values_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_clean_data(names_list,details_list):\n",
    "    \"\"\"\n",
    "    Combines the names and details into a single list of lists.\n",
    "    Dealing with them separately is difficult to follow so I want to combine them ASAP\n",
    "    \n",
    "    Returns single list of lists\n",
    "    \"\"\"\n",
    "    for i, val in enumerate(details_list):\n",
    "        val.insert(0, names_list[i])\n",
    "        \n",
    "    return details_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_into_dataframe(x):\n",
    "    \"\"\"\n",
    "    Takes two lists, the urls and the raw data, and puts them into a pandas dataframe.\n",
    "    \n",
    "    x is URLs\n",
    "    y is raw data\n",
    "    \n",
    "    \"\"\"\n",
    "    # First have to check the lengths of the lists in the list\n",
    "    for i, val in enumerate(x):\n",
    "        val.extend([float(\"NaN\")]*(7-len(val)))\n",
    "    \n",
    "    df = pd.DataFrame(data = x, columns = [\"profile_url\",\"member_since\",\"earnings_pst_yr\",\"earnings_ever\",\n",
    "                                       \"employers\",\"invoices_paid\",\"largest_employ\"])\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_table_to_db(dataframe,table_name):\n",
    "    \"\"\"\n",
    "    Adds the data to a new table (details_table) in freelance_db.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Try to figure out how to put these into a config file later.\n",
    "    dbname = 'freelance_db'\n",
    "    username = 'Metaverse'\n",
    "    pswd = 'arcifice91'\n",
    "    \n",
    "    # Connect to the database\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    print('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "\n",
    "    ## insert data into database from Python (proof of concept - this won't be useful for big data, of course)\n",
    "    ## df is any pandas dataframe \n",
    "    dataframe.to_sql(table_name, engine, if_exists='replace')\n",
    "    \n",
    "    print(\"Added data to %s\"%(dbname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://Metaverse:arcifice91@localhost/freelance_db\n",
      "Added data to freelance_db\n"
     ]
    }
   ],
   "source": [
    "# Putting it all together\n",
    "# Creating list of urls\n",
    "html_core = \"https://www.guru.com/d/freelancers/ls/united-states/california/srt/rv-h/pg/\"\n",
    "pg_nums = list(map(str,list(range(1,945))))\n",
    "tmp = [s + \"/\" for s in pg_nums]\n",
    "htmls = [html_core + s for s in tmp]\n",
    "\n",
    "# Scraping\n",
    "for j, url in enumerate(htmls[0:10]):\n",
    "        \n",
    "    raw_html = details_scrape(url)\n",
    "    \n",
    "    # If the page doesn't load just skip it for now\n",
    "    if raw_html == None:\n",
    "        continue\n",
    "\n",
    "    user_urls_soup = raw_to_soup(raw_html[0])\n",
    "    user_details_soup = raw_to_soup(raw_html[1])\n",
    "\n",
    "    user_urls_clean = soup_urls_to_list(user_urls_soup)\n",
    "    user_details_clean = soup_details_to_list(user_details_soup)\n",
    "\n",
    "    combined_data = combine_clean_data(user_urls_clean, user_details_clean)\n",
    "\n",
    "    if j == 0:\n",
    "        df_selen = combine_into_dataframe(combined_data)\n",
    "        df_selen.fillna(value=np.nan, inplace=True)\n",
    "    else:\n",
    "        tmp = combine_into_dataframe(combined_data)\n",
    "        tmp.fillna(value=np.nan, inplace=True)\n",
    "        df_selen = pd.concat([df_selen,tmp])\n",
    "\n",
    "add_table_to_db(df_selen,\"user_details_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200 entries, 0 to 19\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   profile_url      200 non-null    object\n",
      " 1   member_since     125 non-null    object\n",
      " 2   earnings_pst_yr  125 non-null    object\n",
      " 3   earnings_ever    125 non-null    object\n",
      " 4   employers        125 non-null    object\n",
      " 5   invoices_paid    125 non-null    object\n",
      " 6   largest_employ   125 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 12.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_selen.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
