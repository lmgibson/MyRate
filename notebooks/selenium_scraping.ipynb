{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Dynamic HTML using Selenium\n",
    "\n",
    "There are sections of the website that have to be interacted with in order to access them. This code exists to scrape just those sections. For code that scrapes the static elements of the website please see scraping_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Data Management and SQL Access Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions for Scrape\n",
    "\n",
    "1. Extract elements from two sub-menus: details and about\n",
    "2. Extract HTML from those elements and save into separate lists.\n",
    "3. Extract data from the HTMLs, still have information in two separate lists.\n",
    "4. Combine extracted information into single database\n",
    "5. Figure out which button to click to go to next page (faster than reloading a new URL)\n",
    "6. Finish scraping. Save results to SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_form_click():\n",
    "    \"\"\"\n",
    "    Occasionally the locations of the buttons will break. This updates\n",
    "    their locations and then clicks them, if it is broken.\n",
    "    \n",
    "    Returns: Nothing. Just clicks the button.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        login_form[2].click()\n",
    "    except:\n",
    "        login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "        login_form[2].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details_about_scrape():\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        - Uses selenium to scrape additional information contained in a sub-menu\n",
    "        - Clicks the first box, guru throws up a log-in, goes back, opens all the boxes\n",
    "          and then finishes by extracting all the html from each box.\n",
    "    \n",
    "    Inputs:\n",
    "        - This takes in a single URL (string) and extracts the HTML for the sub-menus. \n",
    "        \n",
    "    Returns:\n",
    "        - user_names: profile_url names to identify each user, cleaned string.\n",
    "        - detail_html: detailed information on their activity on the website, html code.\n",
    "        - about_html: Bio information, html code.\n",
    "    \"\"\"\n",
    "    first_url = driver.current_url\n",
    "    login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "    \n",
    "    # First, checking if the site loaded. If it didn't the code will error out.\n",
    "    # The reason for this is usually if the site doesn't load than user intervention\n",
    "    # is required to reset by filling out a captcha.\n",
    "    if len(login_form) == 0:\n",
    "        return None\n",
    "    \n",
    "    time.sleep(2) # Wait to click so everything can load.\n",
    "    login_form_click()\n",
    "\n",
    "    # Fix for when the website asks for a log-in. Works by checking if the URL has changed\n",
    "    # and if it has changed it will simply page back. Otherwise it continues on by opening\n",
    "    # the boxes. \n",
    "    if driver.current_url != first_url:\n",
    "        time.sleep(2) # Wait for log-in prompt\n",
    "        driver.back() # Go back to main page\n",
    "        time.sleep(1) # Wait to scrape\n",
    "        login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "    else:\n",
    "        login_form_click()\n",
    "    \n",
    "    # Every user has 4 buttons. This clicks the 3rd button for each user and opens\n",
    "    # the sub-menu.\n",
    "    counter = 2\n",
    "    for i, val in enumerate(login_form): \n",
    "        #loop one\n",
    "        if i == counter:\n",
    "            login_form[i].click()\n",
    "            counter += 4\n",
    "    \n",
    "    # Pull elements in the now opened detail boxes\n",
    "    # Extracts the user_name (for reference and later merging)\n",
    "    # and extract the locations of the HTML contained within the opened menu.\n",
    "    user_name = driver.find_elements_by_xpath('//h3[@class=\"freelancerAvatar__screenName\"]')\n",
    "    user_detail = driver.find_elements_by_xpath('//div[@class=\"feedback\"]')\n",
    "\n",
    "    # Extract text from the names and HTML from the details\n",
    "    # Each is a list of length equal to the number of users on the page\n",
    "    names = []\n",
    "    for i, val in enumerate(user_name):\n",
    "        names.append(val.get_attribute('innerHTML'))\n",
    "\n",
    "    detail_html = []\n",
    "    for i, val in enumerate(user_detail):\n",
    "        detail_html.append(val.get_attribute('innerHTML'))\n",
    "        \n",
    "    # Now we go back and get the information in the fourth button (about) menu.\n",
    "    counter = 3\n",
    "    for i, val in enumerate(login_form):    \n",
    "        if i == counter:\n",
    "            login_form[i].click()\n",
    "            counter += 4\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Pull elements and then HTML from the fourth menu (about).\n",
    "    user_about = driver.find_elements_by_xpath('//div[@class=\"profile-about\"]')\n",
    "    about_html = []\n",
    "    for i, val in enumerate(user_about):\n",
    "        about_html.append(val.get_attribute('innerHTML'))\n",
    "            \n",
    "    # Return user names, user details HTML, and user about HTML\n",
    "    return names, detail_html, about_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_soup(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of raw htmls and parses them with BeautifulSoup.\n",
    "    \n",
    "    Returns a list of cleaner HTMLs (soup objects)\n",
    "    \n",
    "    \"\"\"\n",
    "    soups = []\n",
    "    for i, val in enumerate(x):\n",
    "        soups.append(BeautifulSoup(val,'html.parser'))\n",
    "        \n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_urls_to_html_list(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of soups and extracts user htmls from them.\n",
    "    \n",
    "    Returns a list with user htmls\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    user_htmls = []\n",
    "    for i, val in enumerate(x):\n",
    "        user_htmls.append(val.a['href'])\n",
    "        \n",
    "    return user_htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_details_to_html_list(x):\n",
    "    \"\"\"\n",
    "    Takes in the list of details HTMLs and extracts the information from them.\n",
    "    \n",
    "    Returns a list with lists where the first element has a list containing the elements\n",
    "    for the first user on the page, the second contains a list of elements for the second\n",
    "    user details page, and so on.\n",
    "    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for i, val in enumerate(x):\n",
    "        values.append(val.find_all('em'))\n",
    "        \n",
    "    html_to_string = lambda x: x.string\n",
    "    values_strings = []\n",
    "    for i, val in enumerate(values):\n",
    "        values_strings.append(list(map(html_to_string, val)))\n",
    "        \n",
    "        # Lots of empty details pages. This at least fill in an NA.\n",
    "        if len(values_strings[i]) == 0:\n",
    "            values_strings[i].append([\"NA\",\"NA\",\"NA\",\"NA\",\"NA\",\"NA\"])\n",
    "        \n",
    "    return values_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_about_to_html_list(x):\n",
    "    \"\"\"\n",
    "    Takes in the list of about HTMLs for the users on a given page. Extracts the \n",
    "    information in the about section and returns a list the length of the number\n",
    "    of users on the page\n",
    "    \n",
    "    Returns a list of user about information.\n",
    "    \"\"\"\n",
    "    \n",
    "    about_vals = []\n",
    "    for i, val in enumerate(user_about_soup):\n",
    "        try:\n",
    "            tmp = val.find_all('pre')[0].text\n",
    "        except:\n",
    "            tmp = 'NA'\n",
    "        about_vals.append(tmp)\n",
    "        \n",
    "    return about_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_clean_data(names_list,details_list, about_list):\n",
    "    \"\"\"\n",
    "    Combines the names and details into a single list of lists.\n",
    "    Dealing with them separately is difficult to follow so I want to combine them ASAP\n",
    "    \n",
    "    Input:\n",
    "        - names_list: List of profile names (for identification and merging)\n",
    "        - details_list: List of lists where nested list is data for a given user\n",
    "        - about_list: List of bios. \n",
    "    \n",
    "    Returns single list of lists. Where the len of the list is = the number of users, and\n",
    "    the length of the nested list is equal to the number of data columns.\n",
    "    \"\"\"\n",
    "    for i, val in enumerate(details_list):\n",
    "        val.insert(0, names_list[i])\n",
    "        \n",
    "    for i, val in enumerate(details_list):\n",
    "        val.insert(len(val),about_list[i])\n",
    "        \n",
    "    return details_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_into_dataframe(x):\n",
    "    \"\"\"\n",
    "    Convert the details_list returned from combine_clean_data into a pandas\n",
    "    dataframe. \n",
    "    \"\"\"\n",
    "    # First have to check the lengths of the lists in the list\n",
    "    # For lists that are not the full length (missing data) I extend\n",
    "    # it to be equal length. This makes putting it into the pandas data\n",
    "    # frame easier. \n",
    "    for i, val in enumerate(x):\n",
    "        val.extend([float(\"NaN\")]*(8-len(val)))\n",
    "    \n",
    "    # Convert into dataframe\n",
    "    df = pd.DataFrame(data = x, columns = [\"profile_url\",\"member_since\",\"earnings_pst_yr\",\"earnings_ever\",\n",
    "                                       \"employers\",\"invoices_paid\",\"largest_employ\",\"bio\"])\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagination():\n",
    "    \"\"\"\n",
    "    The website doesn't have a 'next' button to change the page. This creates\n",
    "    a list that contains the current page numbers at the bottom of the page.\n",
    "    I use this against the current page number to determine which element to click.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting elements containing page change buttons\n",
    "    a = driver.find_element_by_xpath('//*[@id=\"ctl00_guB_ulpaginate\"]')\n",
    "    \n",
    "    # Extracting the text in each one\n",
    "    soup = BeautifulSoup(a.get_attribute('innerHTML'),'html.parser')\n",
    "    soup.find_all('a')\n",
    "    \n",
    "    # Saving results to list. Compare this against current page num.\n",
    "    page_list = []\n",
    "    for i, val in enumerate(soup):\n",
    "        page_list.append(val.text)\n",
    "        \n",
    "    return page_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_table_to_db(dataframe,table_name):\n",
    "    \"\"\"\n",
    "    Adds the data to a new table (details_table) in freelance_db.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Try to figure out how to put these into a config file later.\n",
    "    dbname = 'freelance_db'\n",
    "    username = os.environ['USER']\n",
    "    pswd = os.environ['SQLPSWD']\n",
    "    \n",
    "    # Connect to the database\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    print('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "\n",
    "    ## insert data into database from Python (proof of concept - this won't be useful for big data, of course)\n",
    "    ## df is any pandas dataframe \n",
    "    dataframe.to_sql(table_name, engine, if_exists='replace')\n",
    "    \n",
    "    print(\"Added data to %s\"%(dbname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing. Scraping and Saving to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished page 89\n",
      "Finished page 90\n",
      "Finished page 91\n",
      "Finished page 92\n",
      "Finished page 93\n",
      "Finished page 94\n",
      "Finished page 95\n",
      "Finished page 96\n",
      "Finished page 97\n",
      "Finished page 98\n",
      "Finished page 99\n",
      "Finished page 100\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "range object index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-0207e88c4386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Changing the page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# First figuring out what page I'm on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mcur_page_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpg_nums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mgoal_page_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_page_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: range object index out of range"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.guru.com/d/freelancers/l/united-states/pg/1/\")\n",
    "\n",
    "pg_nums = range(1,200)\n",
    "# Scraping\n",
    "for j in range(0,500):\n",
    "        \n",
    "    \n",
    "    raw_html = details_about_scrape()\n",
    "    \n",
    "    if len(raw_html[2])!=20:\n",
    "        print(\"OH NO!\")\n",
    "        time.sleep(1)\n",
    "        driver.refresh()\n",
    "        time.sleep(1)\n",
    "        raw_html = details_about_scrape()\n",
    "\n",
    "    user_urls_soup = raw_to_soup(raw_html[0])\n",
    "    user_details_soup = raw_to_soup(raw_html[1])\n",
    "    user_about_soup = raw_to_soup(raw_html[2])\n",
    "\n",
    "    user_urls_clean = soup_urls_to_html_list(user_urls_soup)\n",
    "    user_details_clean = soup_details_to_html_list(user_details_soup)\n",
    "    user_about_clean = soup_about_to_html_list(user_about_soup)\n",
    "\n",
    "    combined_data = combine_clean_data(user_urls_clean, user_details_clean, user_about_clean)\n",
    "\n",
    "    if j == 88:\n",
    "        df_tmp = combine_into_dataframe(combined_data)\n",
    "        df_tmp.fillna(value=np.nan, inplace=True)\n",
    "    else:\n",
    "        tmp = combine_into_dataframe(combined_data)\n",
    "        tmp.fillna(value=np.nan, inplace=True)\n",
    "        df_tmp = pd.concat([df_tmp,tmp])\n",
    "    \n",
    "    print(\"Finished page \" + str(j+1))\n",
    "    \n",
    "    # Changing the page\n",
    "    # First figuring out what page I'm on\n",
    "    cur_page_num = int(pg_nums[j])\n",
    "    goal_page_num = str(cur_page_num+1)\n",
    "\n",
    "    # Then make a list of all page listings at the bottom of the current page\n",
    "    button_directory = pagination()\n",
    "    go_to = button_directory.index(goal_page_num)\n",
    "\n",
    "    # Create URL for the next page that I want to go to.\n",
    "    xpath_click = '/html/body/form/main/main/section/div/div[2]/div[2]/ul/li['+str(go_to+1)+']/a'\n",
    "    driver.find_element_by_xpath(xpath_click).click()\n",
    "\n",
    "driver.close()\n",
    "# Save results to databse\n",
    "# add_table_to_db(df_selen,\"user_details_table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
