{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details_scrape(url):\n",
    "    \"\"\"\n",
    "    Uses selenium to scrape additional information contained in a sub-menu\n",
    "    \n",
    "    Clicks the first box, guru throws up a log-in, goes back, opens all the boxes\n",
    "    and then finishes by extracting all the html from each box.\n",
    "    \n",
    "    This takes in a single URL (string) and extracts from that. One problem that may arise\n",
    "    is if it doesn't throw the log-in box and the driver tries to go back a page.\n",
    "    However, I think we can get around this by just loading each URL new. \n",
    "    \n",
    "    May be more efficient to page through? That's for another time.\n",
    "    \n",
    "    RETURNS: list of names and list of HTML for each details box.\n",
    "    \n",
    "    Test URL:\n",
    "    \"https://www.guru.com/d/freelancers/lc/united-states/california/los-angeles/pg/1/\"\n",
    "    \"\"\"\n",
    "    \n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "\n",
    "    time.sleep(2) # Wait to click\n",
    "    login_form[2].click()\n",
    "\n",
    "    time.sleep(2) # Wait for log-in prompt\n",
    "    driver.back() # Go back to main page\n",
    "    time.sleep(2) # Wait to scrape\n",
    "\n",
    "    # Pulling the paths again (they change for some reason)\n",
    "    login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]') \n",
    "    counter = 2\n",
    "    \n",
    "    # There are multiple buttons so here I am just clicking the ones I want (2,6,10,...)\n",
    "    for i, val in enumerate(login_form):    \n",
    "        if i == counter:\n",
    "            login_form[i].click()\n",
    "            counter += 4\n",
    "    \n",
    "    print(\"Completed opening all the tabs\")\n",
    "\n",
    "    # Pull elements in the now opened detail boxes\n",
    "    user_name = driver.find_elements_by_xpath('//h3[@class=\"freelancerAvatar__screenName\"]')\n",
    "    user_detail = driver.find_elements_by_xpath('//ul[@class=\"feedback__stats clearfix\"]')\n",
    "\n",
    "    # Extract text from the names and HTML from the details\n",
    "    # Will parse the detail_html further with beautiful soup\n",
    "    names = []\n",
    "    for i, val in enumerate(user_name):\n",
    "        names.append(val.text)\n",
    "\n",
    "    detail_html = []\n",
    "    for i, val in enumerate(user_detail):\n",
    "        detail_html.append(val.get_attribute('innerHTML'))\n",
    "\n",
    "    driver.close()\n",
    "    print(\"Finished\")\n",
    "    \n",
    "    return names, detail_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details_to_soup(x):\n",
    "    \n",
    "    soups = []\n",
    "    for i, val in enumerate(x):\n",
    "        soup.append(BeautifulSoup(x,'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li class=\"feedback__stats__stat\">\n",
      " Member since:\n",
      " <em>\n",
      "  Apr, 2019\n",
      " </em>\n",
      "</li>\n",
      "<li class=\"feedback__stats__stat\">\n",
      " Earnings in Past 12 Months:\n",
      " <em>\n",
      "  $23,618\n",
      " </em>\n",
      "</li>\n",
      "<li class=\"feedback__stats__stat\">\n",
      " All Time Earnings:\n",
      " <em>\n",
      "  $23,618\n",
      " </em>\n",
      "</li>\n",
      "<li class=\"feedback__stats__stat\">\n",
      " Employers:\n",
      " <em>\n",
      "  1\n",
      " </em>\n",
      "</li>\n",
      "<li class=\"feedback__stats__stat\">\n",
      " Invoices Paid:\n",
      " <em>\n",
      "  24\n",
      " </em>\n",
      "</li>\n",
      "<li class=\"feedback__stats__stat\">\n",
      " Largest Employer:\n",
      " <em>\n",
      "  $23,618\n",
      " </em>\n",
      "</li>\n"
     ]
    }
   ],
   "source": [
    "# Building beautiful soup to process detail\n",
    "detail = BeautifulSoup(test[1][1], 'html.parser')\n",
    "print(detail.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<em> Apr, 2019</em>, <em> $23,618 </em>, <em>$23,618 </em>, <em>1 </em>, <em>24 </em>, <em> $23,618 </em>]\n"
     ]
    }
   ],
   "source": [
    "print(detail.find_all('em'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
