{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_form_click():\n",
    "    \"\"\"\n",
    "    Bandaid on issue where login_form resets sometimes\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        login_form[2].click()\n",
    "    except:\n",
    "        login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "        login_form[2].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details_scrape():\n",
    "    \"\"\"\n",
    "    Uses selenium to scrape additional information contained in a sub-menu\n",
    "    \n",
    "    Clicks the first box, guru throws up a log-in, goes back, opens all the boxes\n",
    "    and then finishes by extracting all the html from each box.\n",
    "    \n",
    "    This takes in a single URL (string) and extracts from that. One problem that may arise\n",
    "    is if it doesn't throw the log-in box and the driver tries to go back a page.\n",
    "    However, I think we can get around this by just loading each URL new. \n",
    "    \n",
    "    May be more efficient to page through? That's for another time.\n",
    "    \n",
    "    RETURNS: list of names and list of HTML for each details box.\n",
    "    \n",
    "    Test URL:\n",
    "    \"https://www.guru.com/d/freelancers/lc/united-states/california/los-angeles/pg/1/\"\n",
    "    \"\"\"\n",
    "    first_url = driver.current_url\n",
    "    login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "    \n",
    "    # Checking if the site loaded\n",
    "    if len(login_form) == 0:\n",
    "        return None\n",
    "    \n",
    "    time.sleep(2) # Wait to click\n",
    "    login_form_click()\n",
    "\n",
    "    if driver.current_url != first_url:\n",
    "        time.sleep(2) # Wait for log-in prompt\n",
    "        driver.back() # Go back to main page\n",
    "        time.sleep(1) # Wait to scrape\n",
    "        login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "    else:\n",
    "        login_form_click()\n",
    "    \n",
    "    counter = 2\n",
    "    # There are multiple buttons so here I am just clicking the ones I want (2,6,10,...)\n",
    "    for i, val in enumerate(login_form): \n",
    "        #loop one\n",
    "        if i == counter:\n",
    "            login_form[i].click()\n",
    "            counter += 4\n",
    "    \n",
    "    # Pull elements in the now opened detail boxes\n",
    "    user_name = driver.find_elements_by_xpath('//div[@class=\"module_avatar freelancerAvatar\"]')\n",
    "    user_detail = driver.find_elements_by_xpath('//div[@class=\"feedback\"]')\n",
    "\n",
    "    # Extract text from the names and HTML from the details\n",
    "    # Will parse the detail_html further with beautiful soup\n",
    "    # Each is a list of length equal to the number of users on the page\n",
    "    names = []\n",
    "    for i, val in enumerate(user_name):\n",
    "        names.append(val.get_attribute('innerHTML'))\n",
    "\n",
    "    detail_html = []\n",
    "    for i, val in enumerate(user_detail):\n",
    "        detail_html.append(val.get_attribute('innerHTML'))\n",
    "        \n",
    "    # Now let's go get the about page\n",
    "    counter = 3\n",
    "    # There are multiple buttons so here I am just clicking the ones I want (2,6,10,...)\n",
    "    for i, val in enumerate(login_form):    \n",
    "        if i == counter:\n",
    "            login_form[i].click()\n",
    "            counter += 4\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Pull elements in the now opened detail boxes\n",
    "    user_about = driver.find_elements_by_xpath('//div[@class=\"profile-about\"]')\n",
    "    about_html = []\n",
    "    for i, val in enumerate(user_about):\n",
    "        about_html.append(val.get_attribute('innerHTML'))\n",
    "            \n",
    "    return names, detail_html, about_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_soup(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of raw htmls and parses them with BeautifulSoup.\n",
    "    \n",
    "    Returns a list of cleaner HTMLs (soup objects), \n",
    "    \n",
    "    \"\"\"\n",
    "    soups = []\n",
    "    for i, val in enumerate(x):\n",
    "        soups.append(BeautifulSoup(val,'html.parser'))\n",
    "        \n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_urls_to_list(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of soups and extracts user htmls from them.\n",
    "    \n",
    "    Returns a list with user htmls\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    user_htmls = []\n",
    "    for i, val in enumerate(x):\n",
    "        user_htmls.append(val.a['href'])\n",
    "        \n",
    "    return user_htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_details_to_list(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of soups and extracts the em from them.\n",
    "    \n",
    "    Returns a list with lists where the first element has a list containing the em elements.\n",
    "    The second element also has a list containing the em elements, and so on.\n",
    "    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for i, val in enumerate(x):\n",
    "        values.append(val.find_all('em'))\n",
    "        \n",
    "    html_to_string = lambda x: x.string\n",
    "    values_strings = []\n",
    "    for i, val in enumerate(values):\n",
    "        values_strings.append(list(map(html_to_string, val)))\n",
    "        \n",
    "    return values_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_about_to_list(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of soups and extracts the relevant detail.\n",
    "    This is specific to the user about section. This will extract the bio\n",
    "    information and return it as a list of long strings.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    about_vals = []\n",
    "    for i, val in enumerate(user_about_soup):\n",
    "        try:\n",
    "            tmp = val.find_all('pre')[0].text\n",
    "        except:\n",
    "            tmp = 'NA'\n",
    "        about_vals.append(tmp)\n",
    "        \n",
    "    return about_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_clean_data(names_list,details_list, about_list):\n",
    "    \"\"\"\n",
    "    Combines the names and details into a single list of lists.\n",
    "    Dealing with them separately is difficult to follow so I want to combine them ASAP\n",
    "    \n",
    "    Returns single list of lists\n",
    "    \"\"\"\n",
    "    for i, val in enumerate(details_list):\n",
    "        val.insert(0, names_list[i])\n",
    "        \n",
    "    for i, val in enumerate(details_list):\n",
    "        val.insert(len(val),about_list[i])\n",
    "        \n",
    "    return details_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_into_dataframe(x):\n",
    "    \"\"\"\n",
    "    Takes two lists, the urls and the raw data, and puts them into a pandas dataframe.\n",
    "    \n",
    "    x is URLs\n",
    "    y is raw data\n",
    "    \n",
    "    \"\"\"\n",
    "    # First have to check the lengths of the lists in the list\n",
    "    for i, val in enumerate(x):\n",
    "        val.extend([float(\"NaN\")]*(7-len(val)))\n",
    "    \n",
    "    df = pd.DataFrame(data = x, columns = [\"profile_url\",\"member_since\",\"earnings_pst_yr\",\"earnings_ever\",\n",
    "                                       \"employers\",\"invoices_paid\",\"largest_employ\",\"bio\"])\n",
    "    \n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagination():\n",
    "    \"\"\"\n",
    "    Creates a database of where the page numbers will send us.\n",
    "    \"\"\"\n",
    "    \n",
    "    a = driver.find_element_by_xpath('//*[@id=\"ctl00_guB_ulpaginate\"]')\n",
    "    soup = BeautifulSoup(a.get_attribute('innerHTML'),'html.parser')\n",
    "    soup.find_all('a')\n",
    "    page_list = []\n",
    "    for i, val in enumerate(soup):\n",
    "        page_list.append(val.text)\n",
    "        \n",
    "    return page_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_table_to_db(dataframe,table_name):\n",
    "    \"\"\"\n",
    "    Adds the data to a new table (details_table) in freelance_db.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Try to figure out how to put these into a config file later.\n",
    "    dbname = 'freelance_db'\n",
    "    username = 'Metaverse'\n",
    "    pswd = 'arcifice91'\n",
    "    \n",
    "    # Connect to the database\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    print('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "\n",
    "    ## insert data into database from Python (proof of concept - this won't be useful for big data, of course)\n",
    "    ## df is any pandas dataframe \n",
    "    dataframe.to_sql(table_name, engine, if_exists='replace')\n",
    "    \n",
    "    print(\"Added data to %s\"%(dbname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished page 41\n",
      "Finished page 42\n",
      "Finished page 43\n",
      "Finished page 44\n",
      "Finished page 45\n",
      "Finished page 46\n",
      "Finished page 47\n",
      "Finished page 48\n",
      "Finished page 49\n",
      "Finished page 50\n",
      "Finished page 51\n"
     ]
    }
   ],
   "source": [
    "# Putting it all together\n",
    "# Creating list of urls\n",
    "# html_core = \"https://www.guru.com/d/freelancers/l/united-states/pg/\"\n",
    "# pg_nums = list(map(str,list(range(1,945))))\n",
    "# tmp = [s + \"/\" for s in pg_nums]\n",
    "# htmls = [html_core + s for s in tmp]\n",
    "\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.guru.com/d/freelancers/l/united-states/pg/1/\")\n",
    "\n",
    "# Scraping\n",
    "for j in range(0,51):\n",
    "        \n",
    "    \n",
    "    raw_html = details_scrape()\n",
    "    \n",
    "    if len(raw_html[2])==0:\n",
    "        print(\"OH NO!\")\n",
    "        time.sleep(10)\n",
    "        raw_html = details_scrape()\n",
    "\n",
    "    user_urls_soup = raw_to_soup(raw_html[0])\n",
    "    user_details_soup = raw_to_soup(raw_html[1])\n",
    "    user_about_soup = raw_to_soup(raw_html[2])\n",
    "\n",
    "    user_urls_clean = soup_urls_to_list(user_urls_soup)\n",
    "    user_details_clean = soup_details_to_list(user_details_soup)\n",
    "    user_about_clean = soup_about_to_list(user_about_soup)\n",
    "\n",
    "    combined_data = combine_clean_data(user_urls_clean, user_details_clean, user_about_clean)\n",
    "\n",
    "    if j == 0:\n",
    "        df_tmp = combine_into_dataframe(combined_data)\n",
    "        df_tmp.fillna(value=np.nan, inplace=True)\n",
    "    else:\n",
    "        tmp = combine_into_dataframe(combined_data)\n",
    "        tmp.fillna(value=np.nan, inplace=True)\n",
    "        df_tmp = pd.concat([df_tmp,tmp])\n",
    "    \n",
    "    print(\"Finished page \" + str(j+1))\n",
    "    \n",
    "    # Changing the page. Difficult because no next button.\n",
    "        # First figuring out what page I'm on\n",
    "    cur_page_num = int(pg_nums[j])\n",
    "    goal_page_num = str(cur_page_num+1)\n",
    "\n",
    "    # Then make a dataset of all page listings we can reach\n",
    "    button_directory = pagination()\n",
    "    go_to = button_directory.index(goal_page_num)\n",
    "\n",
    "    # Create URL based on current page number\n",
    "    xpath_click = '/html/body/form/main/main/section/div/div[2]/div[2]/ul/li['+str(go_to+1)+']/a'\n",
    "    driver.find_element_by_xpath(xpath_click).click()\n",
    "            \n",
    "add_table_to_db(df_selen,\"user_details_table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
