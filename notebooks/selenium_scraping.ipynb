{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details_scrape(url):\n",
    "    \"\"\"\n",
    "    Uses selenium to scrape additional information contained in a sub-menu\n",
    "    \n",
    "    Clicks the first box, guru throws up a log-in, goes back, opens all the boxes\n",
    "    and then finishes by extracting all the html from each box.\n",
    "    \n",
    "    This takes in a single URL (string) and extracts from that. One problem that may arise\n",
    "    is if it doesn't throw the log-in box and the driver tries to go back a page.\n",
    "    However, I think we can get around this by just loading each URL new. \n",
    "    \n",
    "    May be more efficient to page through? That's for another time.\n",
    "    \n",
    "    RETURNS: list of names and list of HTML for each details box.\n",
    "    \n",
    "    Test URL:\n",
    "    \"https://www.guru.com/d/freelancers/lc/united-states/california/los-angeles/pg/1/\"\n",
    "    \"\"\"\n",
    "    \n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "    first_url = driver.current_url\n",
    "    \n",
    "    time.sleep(2) # Wait to click\n",
    "    login_form[2].click()\n",
    "\n",
    "    if driver.current_url != first_url:\n",
    "        time.sleep(2) # Wait for log-in prompt\n",
    "        driver.back() # Go back to main page\n",
    "        time.sleep(1) # Wait to scrape\n",
    "        login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "    else:\n",
    "        login_form[2].click()\n",
    "\n",
    "    counter = 2\n",
    "    \n",
    "    # There are multiple buttons so here I am just clicking the ones I want (2,6,10,...)\n",
    "    for i, val in enumerate(login_form):    \n",
    "        if i == counter:\n",
    "            login_form[i].click()\n",
    "            counter += 4\n",
    "    \n",
    "    # Pull elements in the now opened detail boxes\n",
    "        # //div[@class=\"avatarinfo\"]\n",
    "    user_name = driver.find_elements_by_xpath('//div[@class=\"module_avatar freelancerAvatar\"]')\n",
    "    user_detail = driver.find_elements_by_xpath('//div[@class=\"feedback\"]')\n",
    "\n",
    "    # Extract text from the names and HTML from the details\n",
    "    # Will parse the detail_html further with beautiful soup\n",
    "    # Each is a list of length equal to the number of users on the page\n",
    "    names = []\n",
    "    for i, val in enumerate(user_name):\n",
    "        names.append(val.get_attribute('innerHTML'))\n",
    "\n",
    "    detail_html = []\n",
    "    for i, val in enumerate(user_detail):\n",
    "        detail_html.append(val.get_attribute('innerHTML'))\n",
    "\n",
    "    driver.close()\n",
    "    \n",
    "    return names, detail_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_soup(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of raw htmls and parses them with BeautifulSoup.\n",
    "    \n",
    "    Returns a list of cleaner HTMLs (soup objects), \n",
    "    \n",
    "    \"\"\"\n",
    "    soups = []\n",
    "    for i, val in enumerate(x):\n",
    "        soups.append(BeautifulSoup(val,'html.parser'))\n",
    "        \n",
    "    return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_urls_to_list(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of soups and extracts user htmls from them.\n",
    "    \n",
    "    Returns a list with user htmls\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    user_htmls = []\n",
    "    for i, val in enumerate(x):\n",
    "        user_htmls.append(val.a['href'])\n",
    "        \n",
    "    return user_htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_details_to_list(x):\n",
    "    \"\"\"\n",
    "    Takes in a list of soups and extracts the em from them.\n",
    "    \n",
    "    Returns a list with lists where the first element has a list containing the em elements.\n",
    "    The second element also has a list containing the em elements, and so on.\n",
    "    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for i, val in enumerate(x):\n",
    "        values.append(val.find_all('em'))\n",
    "        \n",
    "    html_to_string = lambda x: x.string\n",
    "    values_strings = []\n",
    "    for i, val in enumerate(values):\n",
    "        values_strings.append(list(map(html_to_string, val)))\n",
    "        \n",
    "    return values_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_clean_data(names_list,details_list):\n",
    "    \"\"\"\n",
    "    Combines the names and details into a single list of lists.\n",
    "    Dealing with them separately is difficult to follow so I want to combine them ASAP\n",
    "    \n",
    "    Returns single list of lists\n",
    "    \"\"\"\n",
    "    for i, val in enumerate(details_list):\n",
    "        val.insert(0, names_list[i])\n",
    "        \n",
    "    return details_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_into_dataframe(x):\n",
    "    \"\"\"\n",
    "    Takes two lists, the urls and the raw data, and puts them into a pandas dataframe.\n",
    "    \n",
    "    x is URLs\n",
    "    y is raw data\n",
    "    \n",
    "    \"\"\"\n",
    "    # First have to check the lengths of the lists in the list\n",
    "    for i, val in enumerate(x):\n",
    "        val.extend([float(\"NaN\")]*(7-len(val)))\n",
    "    \n",
    "    df = pd.DataFrame(data = x, columns = [\"profile_url\",\"member_since\",\"earnings_pst_yr\",\"earnings_ever\",\n",
    "                                       \"employers\",\"invoices_paid\",\"largest_employ\"])\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_table_to_db(dataframe,table_name):\n",
    "    \"\"\"\n",
    "    Adds the data to a new table (details_table) in freelance_db.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Try to figure out how to put these into a config file later.\n",
    "    dbname = 'freelance_db'\n",
    "    username = 'Metaverse'\n",
    "    pswd = 'arcifice91'\n",
    "    \n",
    "    # Connect to the database\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    print('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "\n",
    "    ## insert data into database from Python (proof of concept - this won't be useful for big data, of course)\n",
    "    ## df is any pandas dataframe \n",
    "    dataframe.to_sql(table_name, engine, if_exists='replace')\n",
    "    \n",
    "    print(\"Added data to %s\"%(dbname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-449-e486dade7806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Scraping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtmls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mraw_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetails_scrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0muser_urls_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_to_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-413-35985d50e23c>\u001b[0m in \u001b[0;36mdetails_scrape\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Wait to click\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mlogin_form\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_url\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfirst_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Putting it all together\n",
    "# Creating list of urls\n",
    "html_core = \"https://www.guru.com/d/freelancers/lc/united-states/california/los-angeles/lc/united-states/california/long-beach/lc/united-states/california/santa-monica/pg/\"\n",
    "pg_nums = list(map(str,list(range(1,945))))\n",
    "tmp = [s + \"/\" for s in pg_nums]\n",
    "htmls = [html_core + s for s in tmp]\n",
    "\n",
    "# Scraping\n",
    "for j, url in enumerate(htmls[0:25]):\n",
    "    raw_html = details_scrape(url)\n",
    "\n",
    "    user_urls_soup = raw_to_soup(raw_html[0])\n",
    "    user_details_soup = raw_to_soup(raw_html[1])\n",
    "\n",
    "    user_urls_clean = soup_urls_to_list(user_urls_soup)\n",
    "    user_details_clean = soup_details_to_list(user_details_soup)\n",
    "\n",
    "    combined_data = combine_clean_data(user_urls_clean, user_details_clean)\n",
    "\n",
    "    if j == 0:\n",
    "        df_selen = combine_into_dataframe(combined_data)\n",
    "        df_selen.fillna(value=np.nan, inplace=True)\n",
    "    else:\n",
    "        tmp = combine_into_dataframe(combined_data)\n",
    "        tmp.fillna(value=np.nan, inplace=True)\n",
    "        df_selen = pd.concat([df_selen,tmp])\n",
    "\n",
    "add_table_to_db(df_selen,\"user_details_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_url</th>\n",
       "      <th>member_since</th>\n",
       "      <th>earnings_pst_yr</th>\n",
       "      <th>earnings_ever</th>\n",
       "      <th>employers</th>\n",
       "      <th>invoices_paid</th>\n",
       "      <th>largest_employ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/freelancers/andre-bernard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/freelancers/crosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/freelancers/wentco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/freelancers/jessie-liu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/freelancers/william-sager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   profile_url member_since earnings_pst_yr earnings_ever  \\\n",
       "15  /freelancers/andre-bernard          NaN             NaN           NaN   \n",
       "16          /freelancers/crosa          NaN             NaN           NaN   \n",
       "17         /freelancers/wentco          NaN             NaN           NaN   \n",
       "18     /freelancers/jessie-liu          NaN             NaN           NaN   \n",
       "19  /freelancers/william-sager          NaN             NaN           NaN   \n",
       "\n",
       "   employers invoices_paid largest_employ  \n",
       "15       NaN           NaN            NaN  \n",
       "16       NaN           NaN            NaN  \n",
       "17       NaN           NaN            NaN  \n",
       "18       NaN           NaN            NaN  \n",
       "19       NaN           NaN            NaN  "
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selen.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
