{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Streamlit Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st  # creating web-app\n",
    "import pandas as pd  # managing data\n",
    "import numpy as np  # managing data for model\n",
    "import pickle  # Importing serialized model for prediction\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords  # Cleaning text data\n",
    "import os\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input_cols():\n",
    "    cols = ['num_skills', 'bio_length', 'bio_word_count', 'avg_word_length', 'num_stop',\n",
    "            'administrative & secretarial', 'business & finance', 'design & art', 'education & training',\n",
    "            'engineering & architecture', 'legal', 'programming & development', 'sales & marketing',\n",
    "            'writing & translation', 'Alabama', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "            'Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n",
    "            'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
    "            'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico',\n",
    "            'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Puerto Rico',\n",
    "            'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',\n",
    "            'Washington', 'West Virginia', 'Wisconsin']\n",
    "\n",
    "    col_dict = dict.fromkeys(cols, 0)\n",
    "\n",
    "    return col_dict\n",
    "\n",
    "\n",
    "# Functions to clean bio data\n",
    "\n",
    "\n",
    "def clean_bio(bio):\n",
    "    try:\n",
    "        cleaned_bio = ''.join(s for s in bio if ord(s) > 31 and ord(s) < 126)\n",
    "    except:\n",
    "        cleaned_bio = \"NaN\"\n",
    "\n",
    "    return cleaned_bio\n",
    "\n",
    "\n",
    "def avg_word_ln(bio):\n",
    "    try:\n",
    "        words = bio.split()\n",
    "        res = (sum(len(word) for word in words) / len(words))\n",
    "    except:\n",
    "        res = 0\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def num_stopwords(bio):\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    try:\n",
    "        res = len([x for x in bio.split() if x in stop])\n",
    "    except:\n",
    "        res = -1\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# Function to upload model\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # Tries the location for the hosted server first. Then tries local.\n",
    "    model_xgboost = xgb.Booster({'nthread': 4})  # init model\n",
    "\n",
    "    try:\n",
    "        filename = '/home/ubuntu/MyRate/scripts/model_xgb.sav'\n",
    "        model_xgboost = pickle.load(open(filename, 'rb'))\n",
    "    except:\n",
    "        filename = '/Users/Metaverse/Desktop/Insight/projects/myrate/scripts/model_xgb.sav'\n",
    "        model_xgboost = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    return model_xgboost\n",
    "\n",
    "# Functions to Create Word2Vec Embedding from example bio\n",
    "# This creates embedding values for a single user.\n",
    "# It works by looping over every word, estimating it's value, and adding it to the feature vector\n",
    "# For words not in the vocabulary it will return a vector of zeros.\n",
    "# After this process we get a matrix that is n_words x 50.\n",
    "# Finally, we collapse this array along the rows by taking the mean to obtain a single\n",
    "# 1x50 embedding vector that can be fed into the model.\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                for tokenized_sentence in corpus]\n",
    "\n",
    "    return np.mean(pd.DataFrame(np.array(features)), axis=0)\n",
    "\n",
    "\n",
    "def get_word_embedding():\n",
    "    # Loading Model\n",
    "    try:\n",
    "        filename = '/home/ubuntu/MyRate/scripts/model_w2v.sav'\n",
    "        model_w2v = pickle.load(open(filename, 'rb'))\n",
    "    except:\n",
    "        filename = os.environ['PWD'] + '/scripts/model_w2v.sav'\n",
    "        model_w2v = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    tokenized_corpus = word_tokenize(bio)\n",
    "    embeddings = averaged_word_vectorizer(corpus=tokenized_corpus, model=model_w2v,\n",
    "                                          num_features=50)\n",
    "\n",
    "    embeddings.index = [str(x) for x in list(embeddings.index)]\n",
    "    embeddings = pd.DataFrame(embeddings).iloc[:, 0].to_dict()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Code to estimate hourly rate\n",
    "def create_input_array():\n",
    "    # Updating Bio Related Features\n",
    "    cols['num_skills'] = 5\n",
    "    cols['bio_length'] = bio_length\n",
    "    cols['bio_word_count'] = bio_word_count\n",
    "    cols['avg_word_length'] = bio_avg_word_length\n",
    "    cols['num_stop'] = bio_num_stop\n",
    "    cols.update(w2v_embedding)\n",
    "\n",
    "    # Updating Location Feature\n",
    "    cols[state] = 1\n",
    "    for i, val in enumerate(skill_categories):\n",
    "        cols[val.lower()] = 1\n",
    "\n",
    "    return pd.DataFrame(cols, index=[0])\n",
    "\n",
    "\n",
    "def estimate_hourly_rate():\n",
    "    # Predicting\n",
    "    pred = model.predict(xgb.DMatrix(cols.values))\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Metaverse/Desktop/Insight/projects/myrate/conda-env/lib/python3.7/site-packages/ipykernel_launcher.py:83: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "bio = \"Scopic is a large web development team\"\n",
    "bio_clean = clean_bio(bio)\n",
    "bio_length = len(bio_clean)\n",
    "bio_avg_word_length = avg_word_ln(bio_clean)\n",
    "bio_num_stop = num_stopwords(bio_clean)\n",
    "bio_word_count = len(str(bio_clean).split(\" \"))\n",
    "w2v_embedding = get_word_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = model_input_cols()\n",
    "state = 'California'\n",
    "skill_categories = ['programming & development']\n",
    "cols = create_input_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.137676], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "estimate_hourly_rate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
