{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Analysis of User Bios\n",
    "References:\n",
    "    - http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    - https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    - https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for PostgreSQL Import and Export\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Data Management and Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Word2Vec\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "\n",
    "# Visualize embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Clustering Embedding Vectors for Documents\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# NLTK for working with text data (cleaning and processing)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Other\n",
    "from itertools import chain\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"freelance_db\"\n",
    "username = os.environ['USER']\n",
    "pswd = os.environ['SQLPSWD']\n",
    "\n",
    "# Connect to Database\n",
    "con = None\n",
    "con = psycopg2.connect(database=dbname, user=username,\n",
    "                       host='localhost', password=pswd)\n",
    "\n",
    "# Extracting table with user profile and bio\n",
    "sql_query = \"\"\"SELECT profile_url, bio from user_details_table;\"\"\"\n",
    "bio_table = pd.read_sql_query(sql_query, con)\n",
    "bio_table = bio_table.dropna() # Removing users with no bio\n",
    "bio_table = bio_table[bio_table.bio != \"NA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = bio_table.bio.tolist()\n",
    "\n",
    "# Create a set of frequent words\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "# Tokenize each document\n",
    "texts = [text.lower().split() for text in text_corpus]\n",
    "\n",
    "# Removing punctuation\n",
    "texts = [[s.translate(str.maketrans('', '', string.punctuation)) for s in word] for word in texts]\n",
    "\n",
    "# Removing stop words\n",
    "texts = [[x for x in words if x not in stoplist] for words in texts]\n",
    "\n",
    "# Count word frequencies\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [\n",
    "    [token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "# Creating Word Dictionary\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "num_features = len(dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert entire corpus to vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Create similarity index\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features = num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing against a new example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'interaction', 'and', 'engineering', 'data', 'c#', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# Testing new example\n",
    "query_document = 'human interaction and engineering data c# analysis'.split()\n",
    "\n",
    "# Converting to vector\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "# Checking similarity of tfidif conversion of vector\n",
    "sims = index[tfidf[query_bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing similarity scores\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2Vec Neural Network\n",
    "\n",
    "The idea here is to take the word vector I previously built and reduce it's dimensionality. Then I can insert that into the model as a new feature. I made a realization that I don't have to have it by users. I can simply put all the sentences together. The point is to train a model.\n",
    "\n",
    "Reference: https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentences from bios\n",
    "data = bio_table.bio.tolist()\n",
    "sentences = [sent_tokenize(text) for text in data]\n",
    "sentences = list(chain.from_iterable(sentences))\n",
    "\n",
    "# Lowercasing each sentence\n",
    "sentences = [w.lower() for w in sentences]\n",
    "\n",
    "# Splitting sentences into words\n",
    "words = [s.split(' ') for s in sentences]\n",
    "\n",
    "# Removing punctuation from each word\n",
    "words = [[w.translate(str.maketrans('', '', string.punctuation)) for w in x] for x in words]\n",
    "\n",
    "# Removing stop words\n",
    "words = [[x for x in s if x not in stoplist] for s in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec on the two sentences\n",
    "size = 50\n",
    "model = gensim.models.Word2Vec(words, min_count=5, size = size, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.index2word\n",
    "wvs = model.wv[words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE(n_components=2, random_state=0, n_iter=5000, perplexity=2)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# T = tsne.fit_transform(wvs)\n",
    "# labels = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "# # for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "# #     plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Text Documents Using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentences from bios\n",
    "data = bio_table.bio.tolist()\n",
    "\n",
    "tokenized_corpus = [word_tokenize(s) for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Metaverse/Desktop/Insight/projects/myrate/conda-env/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.211775</td>\n",
       "      <td>0.082670</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>-0.187576</td>\n",
       "      <td>0.527085</td>\n",
       "      <td>0.027952</td>\n",
       "      <td>-0.373018</td>\n",
       "      <td>0.375317</td>\n",
       "      <td>-0.158288</td>\n",
       "      <td>0.287524</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.887868</td>\n",
       "      <td>0.503113</td>\n",
       "      <td>-0.621936</td>\n",
       "      <td>-0.605516</td>\n",
       "      <td>-0.458044</td>\n",
       "      <td>0.360395</td>\n",
       "      <td>0.230850</td>\n",
       "      <td>0.544939</td>\n",
       "      <td>0.089387</td>\n",
       "      <td>0.189030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.182241</td>\n",
       "      <td>0.072193</td>\n",
       "      <td>-0.133667</td>\n",
       "      <td>-0.160288</td>\n",
       "      <td>0.446433</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>-0.320216</td>\n",
       "      <td>0.321246</td>\n",
       "      <td>-0.137784</td>\n",
       "      <td>0.243430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.756186</td>\n",
       "      <td>0.430652</td>\n",
       "      <td>-0.532264</td>\n",
       "      <td>-0.518236</td>\n",
       "      <td>-0.389219</td>\n",
       "      <td>0.307597</td>\n",
       "      <td>0.196172</td>\n",
       "      <td>0.466388</td>\n",
       "      <td>0.074990</td>\n",
       "      <td>0.163425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.247685</td>\n",
       "      <td>0.091381</td>\n",
       "      <td>-0.178052</td>\n",
       "      <td>-0.212091</td>\n",
       "      <td>0.605762</td>\n",
       "      <td>0.029905</td>\n",
       "      <td>-0.435115</td>\n",
       "      <td>0.435463</td>\n",
       "      <td>-0.184536</td>\n",
       "      <td>0.330945</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.028574</td>\n",
       "      <td>0.587998</td>\n",
       "      <td>-0.718808</td>\n",
       "      <td>-0.705244</td>\n",
       "      <td>-0.528510</td>\n",
       "      <td>0.411946</td>\n",
       "      <td>0.265667</td>\n",
       "      <td>0.635704</td>\n",
       "      <td>0.102416</td>\n",
       "      <td>0.220111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.212179</td>\n",
       "      <td>0.081244</td>\n",
       "      <td>-0.152066</td>\n",
       "      <td>-0.184273</td>\n",
       "      <td>0.519975</td>\n",
       "      <td>0.028526</td>\n",
       "      <td>-0.371839</td>\n",
       "      <td>0.371196</td>\n",
       "      <td>-0.157696</td>\n",
       "      <td>0.279662</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.873660</td>\n",
       "      <td>0.497988</td>\n",
       "      <td>-0.613011</td>\n",
       "      <td>-0.599375</td>\n",
       "      <td>-0.449393</td>\n",
       "      <td>0.352545</td>\n",
       "      <td>0.227080</td>\n",
       "      <td>0.541230</td>\n",
       "      <td>0.086431</td>\n",
       "      <td>0.189738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.209419</td>\n",
       "      <td>0.080943</td>\n",
       "      <td>-0.148384</td>\n",
       "      <td>-0.184141</td>\n",
       "      <td>0.508552</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>-0.366254</td>\n",
       "      <td>0.366537</td>\n",
       "      <td>-0.154483</td>\n",
       "      <td>0.276676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.859970</td>\n",
       "      <td>0.493028</td>\n",
       "      <td>-0.602487</td>\n",
       "      <td>-0.589957</td>\n",
       "      <td>-0.441599</td>\n",
       "      <td>0.347658</td>\n",
       "      <td>0.223794</td>\n",
       "      <td>0.531442</td>\n",
       "      <td>0.086779</td>\n",
       "      <td>0.183669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>-0.198885</td>\n",
       "      <td>0.075583</td>\n",
       "      <td>-0.139745</td>\n",
       "      <td>-0.176276</td>\n",
       "      <td>0.486588</td>\n",
       "      <td>0.024890</td>\n",
       "      <td>-0.344290</td>\n",
       "      <td>0.349625</td>\n",
       "      <td>-0.147055</td>\n",
       "      <td>0.265392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.821497</td>\n",
       "      <td>0.467302</td>\n",
       "      <td>-0.579851</td>\n",
       "      <td>-0.562865</td>\n",
       "      <td>-0.419650</td>\n",
       "      <td>0.335559</td>\n",
       "      <td>0.215934</td>\n",
       "      <td>0.501857</td>\n",
       "      <td>0.084282</td>\n",
       "      <td>0.179124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>-0.177181</td>\n",
       "      <td>0.067949</td>\n",
       "      <td>-0.127467</td>\n",
       "      <td>-0.158215</td>\n",
       "      <td>0.435342</td>\n",
       "      <td>0.022111</td>\n",
       "      <td>-0.311666</td>\n",
       "      <td>0.315038</td>\n",
       "      <td>-0.133195</td>\n",
       "      <td>0.234625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.732457</td>\n",
       "      <td>0.419066</td>\n",
       "      <td>-0.514790</td>\n",
       "      <td>-0.503966</td>\n",
       "      <td>-0.376644</td>\n",
       "      <td>0.296298</td>\n",
       "      <td>0.189689</td>\n",
       "      <td>0.453673</td>\n",
       "      <td>0.071469</td>\n",
       "      <td>0.156430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>-0.116557</td>\n",
       "      <td>0.045219</td>\n",
       "      <td>-0.083370</td>\n",
       "      <td>-0.108826</td>\n",
       "      <td>0.291851</td>\n",
       "      <td>0.015119</td>\n",
       "      <td>-0.207591</td>\n",
       "      <td>0.211861</td>\n",
       "      <td>-0.090320</td>\n",
       "      <td>0.155634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.491826</td>\n",
       "      <td>0.279252</td>\n",
       "      <td>-0.346432</td>\n",
       "      <td>-0.339466</td>\n",
       "      <td>-0.254675</td>\n",
       "      <td>0.199058</td>\n",
       "      <td>0.128540</td>\n",
       "      <td>0.302518</td>\n",
       "      <td>0.050538</td>\n",
       "      <td>0.106629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>-0.234010</td>\n",
       "      <td>0.089930</td>\n",
       "      <td>-0.167795</td>\n",
       "      <td>-0.206796</td>\n",
       "      <td>0.575158</td>\n",
       "      <td>0.028459</td>\n",
       "      <td>-0.410356</td>\n",
       "      <td>0.414230</td>\n",
       "      <td>-0.176367</td>\n",
       "      <td>0.312884</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966784</td>\n",
       "      <td>0.550767</td>\n",
       "      <td>-0.678243</td>\n",
       "      <td>-0.661895</td>\n",
       "      <td>-0.499026</td>\n",
       "      <td>0.389655</td>\n",
       "      <td>0.252732</td>\n",
       "      <td>0.596285</td>\n",
       "      <td>0.097499</td>\n",
       "      <td>0.206240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>-0.160936</td>\n",
       "      <td>0.061406</td>\n",
       "      <td>-0.115215</td>\n",
       "      <td>-0.143220</td>\n",
       "      <td>0.392233</td>\n",
       "      <td>0.020642</td>\n",
       "      <td>-0.281932</td>\n",
       "      <td>0.285159</td>\n",
       "      <td>-0.122207</td>\n",
       "      <td>0.214675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.662843</td>\n",
       "      <td>0.377918</td>\n",
       "      <td>-0.466379</td>\n",
       "      <td>-0.455743</td>\n",
       "      <td>-0.340456</td>\n",
       "      <td>0.267843</td>\n",
       "      <td>0.170751</td>\n",
       "      <td>0.408231</td>\n",
       "      <td>0.064438</td>\n",
       "      <td>0.143115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.211775  0.082670 -0.155556 -0.187576  0.527085  0.027952 -0.373018   \n",
       "1   -0.182241  0.072193 -0.133667 -0.160288  0.446433  0.022265 -0.320216   \n",
       "2   -0.247685  0.091381 -0.178052 -0.212091  0.605762  0.029905 -0.435115   \n",
       "3   -0.212179  0.081244 -0.152066 -0.184273  0.519975  0.028526 -0.371839   \n",
       "4   -0.209419  0.080943 -0.148384 -0.184141  0.508552  0.025275 -0.366254   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "780 -0.198885  0.075583 -0.139745 -0.176276  0.486588  0.024890 -0.344290   \n",
       "781 -0.177181  0.067949 -0.127467 -0.158215  0.435342  0.022111 -0.311666   \n",
       "782 -0.116557  0.045219 -0.083370 -0.108826  0.291851  0.015119 -0.207591   \n",
       "783 -0.234010  0.089930 -0.167795 -0.206796  0.575158  0.028459 -0.410356   \n",
       "784 -0.160936  0.061406 -0.115215 -0.143220  0.392233  0.020642 -0.281932   \n",
       "\n",
       "           7         8         9   ...        40        41        42  \\\n",
       "0    0.375317 -0.158288  0.287524  ... -0.887868  0.503113 -0.621936   \n",
       "1    0.321246 -0.137784  0.243430  ... -0.756186  0.430652 -0.532264   \n",
       "2    0.435463 -0.184536  0.330945  ... -1.028574  0.587998 -0.718808   \n",
       "3    0.371196 -0.157696  0.279662  ... -0.873660  0.497988 -0.613011   \n",
       "4    0.366537 -0.154483  0.276676  ... -0.859970  0.493028 -0.602487   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "780  0.349625 -0.147055  0.265392  ... -0.821497  0.467302 -0.579851   \n",
       "781  0.315038 -0.133195  0.234625  ... -0.732457  0.419066 -0.514790   \n",
       "782  0.211861 -0.090320  0.155634  ... -0.491826  0.279252 -0.346432   \n",
       "783  0.414230 -0.176367  0.312884  ... -0.966784  0.550767 -0.678243   \n",
       "784  0.285159 -0.122207  0.214675  ... -0.662843  0.377918 -0.466379   \n",
       "\n",
       "           43        44        45        46        47        48        49  \n",
       "0   -0.605516 -0.458044  0.360395  0.230850  0.544939  0.089387  0.189030  \n",
       "1   -0.518236 -0.389219  0.307597  0.196172  0.466388  0.074990  0.163425  \n",
       "2   -0.705244 -0.528510  0.411946  0.265667  0.635704  0.102416  0.220111  \n",
       "3   -0.599375 -0.449393  0.352545  0.227080  0.541230  0.086431  0.189738  \n",
       "4   -0.589957 -0.441599  0.347658  0.223794  0.531442  0.086779  0.183669  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "780 -0.562865 -0.419650  0.335559  0.215934  0.501857  0.084282  0.179124  \n",
       "781 -0.503966 -0.376644  0.296298  0.189689  0.453673  0.071469  0.156430  \n",
       "782 -0.339466 -0.254675  0.199058  0.128540  0.302518  0.050538  0.106629  \n",
       "783 -0.661895 -0.499026  0.389655  0.252732  0.596285  0.097499  0.206240  \n",
       "784 -0.455743 -0.340456  0.267843  0.170751  0.408231  0.064438  0.143115  \n",
       "\n",
       "[785 rows x 50 columns]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# get document level embeddings\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=model,\n",
    "                                             num_features=size)\n",
    "pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster using K Means and Extract Labels\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(w2v_feature_array)\n",
    "labels = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_url</th>\n",
       "      <th>bio</th>\n",
       "      <th>Cluster Labels</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/freelancers/scopic</td>\n",
       "      <td>Scopic is a U.S. based company specializing in...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/freelancers/top-guru-assistants</td>\n",
       "      <td>Top Guru Assistants is a team of 100+ professi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/freelancers/eden-programming-village</td>\n",
       "      <td>Our team has many software developers and desi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/freelancers/avion-technology-inc-chicago</td>\n",
       "      <td>A Chicago-based company provides web design, w...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/freelancers/j-consulting</td>\n",
       "      <td>Experienced solutions developer with Masters D...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 profile_url  \\\n",
       "0                        /freelancers/scopic   \n",
       "1           /freelancers/top-guru-assistants   \n",
       "2      /freelancers/eden-programming-village   \n",
       "3  /freelancers/avion-technology-inc-chicago   \n",
       "4                  /freelancers/j-consulting   \n",
       "\n",
       "                                                 bio  Cluster Labels  cluster  \n",
       "0  Scopic is a U.S. based company specializing in...               4        4  \n",
       "1  Top Guru Assistants is a team of 100+ professi...               0        0  \n",
       "2  Our team has many software developers and desi...               1        1  \n",
       "3  A Chicago-based company provides web design, w...               4        4  \n",
       "4  Experienced solutions developer with Masters D...               4        4  "
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_table['cluster'] = labels\n",
    "bio_table.groupby(['cluster']).cluster.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
