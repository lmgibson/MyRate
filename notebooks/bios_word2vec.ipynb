{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Analysis of User Bios\n",
    "References:\n",
    "    - http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    - https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    - https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for PostgreSQL Import and Export\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Data Management and Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Word2Vec\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "\n",
    "# Visualize embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Clustering Embedding Vectors for Documents\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# NLTK for working with text data (cleaning and processing)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Other\n",
    "from itertools import chain\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"freelance_db\"\n",
    "username = os.environ['USER']\n",
    "pswd = os.environ['SQLPSWD']\n",
    "\n",
    "# Connect to Database\n",
    "con = None\n",
    "con = psycopg2.connect(database=dbname, user=username,\n",
    "                       host='localhost', password=pswd)\n",
    "\n",
    "# Extracting table with user profile and bio\n",
    "sql_query = \"\"\"SELECT profile_url, bio from user_details_table;\"\"\"\n",
    "bio_table = pd.read_sql_query(sql_query, con)\n",
    "bio_table = bio_table.dropna() # Removing users with no bio\n",
    "bio_table = bio_table.loc[bio_table.bio != \"NA\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = bio_table.bio.tolist()\n",
    "\n",
    "# Create a set of frequent words\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "# Tokenize each document\n",
    "texts = [text.lower().split() for text in text_corpus]\n",
    "\n",
    "# Removing punctuation\n",
    "texts = [[s.translate(str.maketrans('', '', string.punctuation)) for s in word] for word in texts]\n",
    "\n",
    "# Removing stop words\n",
    "texts = [[x for x in words if x not in stoplist] for words in texts]\n",
    "\n",
    "# Count word frequencies\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [\n",
    "    [token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "# Creating Word Dictionary\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "num_features = len(dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert entire corpus to vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Create similarity index\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features = num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing against a new example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'interaction', 'and', 'engineering', 'data', 'c#', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# Testing new example\n",
    "query_document = 'human interaction and engineering data c# analysis'.split()\n",
    "\n",
    "# Converting to vector\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "# Checking similarity of tfidif conversion of vector\n",
    "sims = index[tfidf[query_bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing similarity scores\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining a Word2Vec Embedding\n",
    "\n",
    "The idea here is to take the word vector I previously built and reduce it's dimensionality. Then I can insert that into the model as a new feature. I made a realization that I don't have to have it by users. I can simply put all the sentences together. The point is to train a model.\n",
    "\n",
    "Reference: https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentences from bios\n",
    "data = bio_table.bio.tolist()\n",
    "sentences = [sent_tokenize(text) for text in data]\n",
    "sentences = list(chain.from_iterable(sentences))\n",
    "\n",
    "# Lowercasing each sentence\n",
    "sentences = [w.lower() for w in sentences]\n",
    "\n",
    "# Splitting sentences into words\n",
    "words = [s.split(' ') for s in sentences]\n",
    "\n",
    "# Removing punctuation from each word\n",
    "words = [[w.translate(str.maketrans('', '', string.punctuation)) for w in x] for x in words]\n",
    "\n",
    "# Removing stop words\n",
    "stoplist = stopwords.words('english')\n",
    "words = [[x for x in s if x not in stoplist] for s in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec\n",
    "size = 100 # Don't go higher than 10% 100 or 200 will be sufficient; Check overfitting drop between train and test.\n",
    "model = gensim.models.Word2Vec(words, min_count=5, size = size, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('development', 0.9999358057975769),\n",
       " ('software', 0.9999181628227234),\n",
       " ('website', 0.9999127388000488),\n",
       " ('design', 0.9999101161956787),\n",
       " ('application', 0.9999095797538757)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['web'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.index2word\n",
    "wvs = model.wv[words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentences from bios\n",
    "data = bio_table.bio.tolist()\n",
    "\n",
    "tokenized_corpus = [word_tokenize(s) for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Metaverse/Desktop/Insight/projects/myrate/conda-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# This extracts the embeddings at the sentence level and then averages across sentences within\n",
    "# a document in order to obtain document (user) level embedding vector\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "# Extracting Datafrom \n",
    "words = model.wv.index2word\n",
    "wvs = model.wv[words]\n",
    "\n",
    "# get document level embeddings\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=model,\n",
    "                                             num_features=size)\n",
    "embeddings = pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Embeddings to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding profile_urls for merging later\n",
    "embeddings['profile_url'] = bio_table.profile_url\n",
    "\n",
    "# Connect to the database and save data to it\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s' %\n",
    "                       (username, pswd, dbname))\n",
    "embeddings.to_sql(\"embeddings_table\", engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE(n_components=2, random_state=0, n_iter=5000, perplexity=2)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# T = tsne.fit_transform(wvs)\n",
    "# labels = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
    "# # for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
    "# #     plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Text Documents Using Word Embeddings\n",
    "\n",
    "This really doesn't seem to be working.\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "\n",
    "Notes from Office Hours with Rachel WK:\n",
    "    - Evaluate with intra-cluster similiarity scores (also between-cluster)\n",
    "    - Manually inspect clusters and get an idea if the things go together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster using K Means and Extract Labels\n",
    "# elbow method (look it up)\n",
    "# Assumes cluster sizes are the same (try DBSCAN)\n",
    "dbscan_model = DBSCAN(eps=2, min_samples=2, n_jobs = 4).fit(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels and inertia \n",
    "labels = dbscan_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "0    785\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding cluster labels to data\n",
    "bio_table['cluster'] = labels\n",
    "bio_table.groupby(['cluster']).cluster.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = bio_table[bio_table.cluster == 19].bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
