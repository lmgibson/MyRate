{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys - broken, can't even access PROJECT_DIR in terminal\n",
    "import os\n",
    "\n",
    "project_dir = os.getenv(\"PROJECT_DIR\")\n",
    "env = os.getenv(\"CONDA_DEFAULT_ENV\")\n",
    "\n",
    "username = os.getenv(\"DATABASE_USR\")\n",
    "pswd = os.getenv(\"DATABASE_PASS\")\n",
    "dbname = os.getenv(\"DATABASE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "\n",
    "# Packages for cleaning Data\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Packages for PostgreSQL Import\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_extract(x):\n",
    "    \"\"\"\n",
    "    Extracts and cleans the html from a website\n",
    "    \n",
    "    Returns soup object from beautiful soup\n",
    "    \"\"\"\n",
    "    source =  requests.get(x).text\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freelancer_extraction(x):\n",
    "    \"\"\"\n",
    "    Extracts freelancer information from a webpage.\n",
    "    \n",
    "    x is a parsed soup object\n",
    "    \n",
    "    Returns a list of freelancers. Requires further cleaning.\n",
    "    See header_content_extraction for next steps.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # extracts the section that contains the freelancer data\n",
    "    a = x.body.form.main.main.section.find_all('ul')[1] \n",
    "\n",
    "    # Create a list where each element is a freelancer.\n",
    "    # The element has a number of different tags for each data point.\n",
    "    b = a.find_all('div',class_ = \"record__details\")\n",
    "    \n",
    "    return b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_content_extraction(x):\n",
    "    \"\"\"\n",
    "    Extract header and content tags from a given freelancer.\n",
    "    \n",
    "    x is a single freelancers HTML information\n",
    "    \n",
    "    Returns a header and content object.\n",
    "        header: contains un-processed information on name, url, location\n",
    "        content: contains un-processed information on skills, rate, description\n",
    "    \n",
    "    \"\"\"\n",
    "    header = x.div\n",
    "    content = x.div.next_sibling.next_sibling\n",
    "    \n",
    "    return header, content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_data_extract(x):\n",
    "    \"\"\"\n",
    "    Extract data from header.\n",
    "    \n",
    "    x is the header for a given freelancer.\n",
    "    \n",
    "    Data being extracted:\n",
    "        - profile url\n",
    "        - city\n",
    "        - state\n",
    "        - country\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting data\n",
    "    profile_url = x.a['href']\n",
    "    \n",
    "    city = x.find('span',class_=\"freelancerAvatar__location--city\").string\n",
    "    state = x.find('span',class_=\"freelancerAvatar__location--state\").string\n",
    "    country = x.find('span',class_=\"freelancerAvatar__location--country\").string\n",
    "    \n",
    "    try:\n",
    "        rating = x.find('span', class_=\"freelancerAvatar__feedback\").text\n",
    "    except:\n",
    "        rating = \"NA\"\n",
    "        \n",
    "    try:\n",
    "        earnings = x.find('span', class_=\"freelancerAvatar__earnings\").text\n",
    "    except:\n",
    "        earnings = \"NA\"\n",
    "        \n",
    "    # Clearning commas off city and state\n",
    "    city = city.replace(',','')\n",
    "    state = state.replace(',','')\n",
    "    \n",
    "    header = {\"profile_url\": profile_url, \"city\": city,\n",
    "              \"state\": state, \"country\": country, \"rating\": rating,\n",
    "              \"earnings\": earnings}\n",
    "    \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_data_extract(x):\n",
    "    \"\"\"\n",
    "    Extracting data from the content\n",
    "    x is the content tag for a given freelancer\n",
    "    \n",
    "    Data being extracted:\n",
    "        - Rates\n",
    "        - Shortened user description (need to go into their profile for detail)\n",
    "        - Skills list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    rates = x.find_all('p')[0].string\n",
    "    user_description_short = x.find_all('p')[1].string\n",
    "    skills_list = x.find_all('a', class_=\"skillsList__skill skillsList__skill--hasHover\")\n",
    "                                   \n",
    "    # Cleaning skills_list from messy html list to list of clean strings\n",
    "    string_clean = lambda skills_list: skills_list.string\n",
    "    skills_list_strings = list(map(string_clean, skills_list))\n",
    "                                   \n",
    "    # Cleaning rates\n",
    "    p = re.compile(r'\\d+')\n",
    "    result = p.findall(rates)\n",
    "    hourly_rate = int(result[0]) # First number is always hourly rate\n",
    "    \n",
    "    # Cleaning user description of indents and return\n",
    "    user_description_short = user_description_short.replace('\\t','')\n",
    "    user_description_short = user_description_short.replace('\\r','')\n",
    "    user_description_short = user_description_short.replace('\\n','')\n",
    "    \n",
    "    # Creating Dictionary\n",
    "    content = {\"hourly_rate\": hourly_rate, \"skills_list\": skills_list_strings,\n",
    "               \"user_description\": user_description_short}\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_table_to_db(dataframe,table_name):\n",
    "    \"\"\"\n",
    "    Adds the data to a new table (details_table) in freelance_db.\n",
    "    \n",
    "    \"\"\"\n",
    "    dbname = \"freelance_db\"\n",
    "    username = \"Metaverse\"\n",
    "    pswd = \"arcifice91\"\n",
    "    \n",
    "    # Connect to the database and save data to it\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    dataframe.to_sql(table_name, engine, if_exists='replace')\n",
    "    \n",
    "    print(\"Added data to %s\"%(dbname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of urls\n",
    "html_core = \"https://www.guru.com/d/freelancers/l/united-states/pg/\"\n",
    "pg_nums = list(map(str,list(range(1,945))))\n",
    "tmp = [s + \"/\" for s in pg_nums]\n",
    "htmls = [html_core + s for s in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Putting it all together\n",
    "df = pd.DataFrame(columns = [\"profile_url\",\"city\",\"state\",\"country\",\"rating\",\"earnings\",\"hourly_rate\",\"skills_list\",\"user_description\"])\n",
    "\n",
    "for k, page in enumerate(htmls[0:51]):\n",
    "    \n",
    "    print(k)\n",
    "        \n",
    "    soup = html_extract(page)\n",
    "    freelancers = freelancer_extraction(soup) # Clean HTML\n",
    "\n",
    "    for k, value in enumerate(freelancers):\n",
    "\n",
    "        header_content = header_content_extraction(value) # Extract two boxes of interest\n",
    "\n",
    "        results = header_data_extract(header_content[0]) # Extract header data\n",
    "        content = content_data_extract(header_content[1]) # Extract content data\n",
    "        results.update(content) # Combine into one dictionary\n",
    "        results = pd.DataFrame(results)\n",
    "        df = df.append(results)\n",
    "        \n",
    "add_table_to_db(df,\"freelance_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
