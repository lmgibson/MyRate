{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping static HTML from Guru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "\n",
    "# Packages for cleaning Data\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Packages for PostgreSQL Import\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Scraping Functions\n",
    "\n",
    "1. The functions below extract the HTML from a webpage, then extract the HTML for each freelancer on the page saving their information into a header and a content variable (header_content_extraction). \n",
    "2. After that the functions split into a function that further extracts information from the header variable and another that extracts more information from the content variable. \n",
    "3. Then the results are combined into a single dictionary and appended to a dataframe that saves the results for every single freelancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_extract(x):\n",
    "    \"\"\"\n",
    "    Extracts and cleans the html from a website (x)\n",
    "    \n",
    "    Returns soup object from beautiful soup\n",
    "    \"\"\"\n",
    "    source =  requests.get(x).text\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freelancer_extraction(x):\n",
    "    \"\"\"\n",
    "    Extracts freelancer information from a soup object returned by html_extract(X)\n",
    "    \n",
    "    Returns a list of freelancers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # extracts the section that contains the freelancer data\n",
    "    a = x.body.form.main.main.section.find_all('ul')[1] \n",
    "\n",
    "    # Create a list where each element is a freelancer.\n",
    "    # The element has a number of different tags for each data point.\n",
    "    b = a.find_all('div',class_ = \"record__details\")\n",
    "    \n",
    "    return b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_content_extraction(x):\n",
    "    \"\"\"\n",
    "    Extract header and content tags for a given freelancer (x).\n",
    "        \n",
    "    Returns a header and content object.\n",
    "        header: contains un-processed information on name, url, location\n",
    "        content: contains un-processed information on skills, rate, description\n",
    "    \"\"\"\n",
    "    header = x.div\n",
    "    content = x.div.next_sibling.next_sibling\n",
    "    \n",
    "    return header, content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_data_extract(x):\n",
    "    \"\"\"\n",
    "    Extract data from the header for a given freelancer.\n",
    "        \n",
    "    Returns the following data in a dictionary:\n",
    "        - profile url\n",
    "        - city\n",
    "        - state\n",
    "        - country\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting data\n",
    "    profile_url = x.a['href']\n",
    "    \n",
    "    city = x.find('span',class_=\"freelancerAvatar__location--city\").string\n",
    "    state = x.find('span',class_=\"freelancerAvatar__location--state\").string\n",
    "    country = x.find('span',class_=\"freelancerAvatar__location--country\").string\n",
    "    \n",
    "    # The feedback section can be missing if a user has never received feedback.\n",
    "    # This try/except clause prevents sets rating to NA if it is blank.\n",
    "    try:\n",
    "        rating = x.find('span', class_=\"freelancerAvatar__feedback\").text\n",
    "    except:\n",
    "        rating = \"NA\"\n",
    "    \n",
    "    # Same thing goes for earnings. However, someone may have earnings but not feedback.\n",
    "    # Therefore, they need to exist in separate try/except clauses.\n",
    "    try:\n",
    "        earnings = x.find('span', class_=\"freelancerAvatar__earnings\").text\n",
    "    except:\n",
    "        earnings = \"NA\"\n",
    "        \n",
    "    # Clearning commas off city and state\n",
    "    city = city.replace(',','')\n",
    "    state = state.replace(',','')\n",
    "    \n",
    "    # Saving into a dictionary. This will later be combined with the content dict.\n",
    "    header = {\"profile_url\": profile_url, \"city\": city,\n",
    "              \"state\": state, \"country\": country, \"rating\": rating,\n",
    "              \"earnings\": earnings}\n",
    "    \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_data_extract(x):\n",
    "    \"\"\"\n",
    "    Extracting data from the content\n",
    "    x is the content tag for a given freelancer\n",
    "    \n",
    "    Returns the following data in a dictionary:\n",
    "        - Rates\n",
    "        - Shortened user description (need to go into their profile for detail)\n",
    "        - Skills list\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting data from the soup HTML object\n",
    "    rates = x.find_all('p')[0].string\n",
    "    user_description_short = x.find_all('p')[1].string\n",
    "    \n",
    "    # Skills is a LIST of up to five elements.\n",
    "    skills_list = x.find_all('a', class_=\"skillsList__skill skillsList__skill--hasHover\")\n",
    "                                   \n",
    "    # Cleaning skills_list from messy html list to list of clean strings\n",
    "    # The lambda lambda function is applying a function to each element in the list.\n",
    "    # There may be a better way to do this? All in one line? List comprehension?\n",
    "    string_clean = lambda skills_list: skills_list.string\n",
    "    skills_list_strings = list(map(string_clean, skills_list))\n",
    "                                   \n",
    "    # Cleaning rates\n",
    "    p = re.compile(r'\\d+')\n",
    "    result = p.findall(rates)\n",
    "    hourly_rate = int(result[0]) # First number is always hourly rate\n",
    "    \n",
    "    # Cleaning user description of indents and return\n",
    "    user_description_short = user_description_short.replace('\\t','')\n",
    "    user_description_short = user_description_short.replace('\\r','')\n",
    "    user_description_short = user_description_short.replace('\\n','')\n",
    "    \n",
    "    # Creating Dictionary. This will be combined with header.\n",
    "    content = {\"hourly_rate\": hourly_rate, \"skills_list\": skills_list_strings,\n",
    "               \"user_description\": user_description_short}\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_table_to_db(dataframe,table_name):\n",
    "    \"\"\"\n",
    "    Adds the data to a new table (details_table) in freelance_db.\n",
    "    inputs:\n",
    "        - dataframe: data you want to save to the databse\n",
    "        - table_name: The name you want to give the data in the database.\n",
    "    \n",
    "    Doesn't return anything other than a message of completion.\n",
    "    \"\"\"\n",
    "    dbname = \"freelance_db\"\n",
    "    username = os.environ['USER']\n",
    "    pswd = os.environ['SQLPSWD']\n",
    "    \n",
    "    # Connect to the database and save data to it\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    dataframe.to_sql(table_name, engine, if_exists='replace')\n",
    "    \n",
    "    print(\"Added data to %s\"%(dbname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of urls to scrape\n",
    "html_core = \"https://www.guru.com/d/freelancers/l/united-states/pg/\"\n",
    "pg_nums = list(map(str,list(range(1,945))))\n",
    "tmp = [s + \"/\" for s in pg_nums]\n",
    "htmls = [html_core + s for s in tmp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Functions, Scraping, and Saving to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing empty dataframe to save results into\n",
    "df = pd.DataFrame(columns=[\"profile_url\", \"city\", \"state\", \"country\",\n",
    "                           \"rating\", \"earnings\", \"hourly_rate\", \"skills_list\", \n",
    "                           \"user_description\"])\n",
    "\n",
    "# Looping over each URL and applying the functions, defined above,\n",
    "# In the order they are written.\n",
    "for k, page in enumerate(htmls[0:51]):\n",
    "\n",
    "    soup = html_extract(page)\n",
    "    freelancers = freelancer_extraction(soup)  # Clean HTML\n",
    "\n",
    "    for k, value in enumerate(freelancers):\n",
    "\n",
    "        header_content = header_content_extraction(\n",
    "            value)  # Extract two boxes of interest\n",
    "\n",
    "        results = header_data_extract(header_content[0])  # Extract header data\n",
    "        content = content_data_extract(\n",
    "            header_content[1])  # Extract content data\n",
    "        results.update(content)  # Combine into one dictionary\n",
    "        results = pd.DataFrame(results)\n",
    "        df = df.append(results)\n",
    "\n",
    "add_table_to_db(df, \"freelance_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
