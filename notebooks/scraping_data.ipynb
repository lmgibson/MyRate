{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "from selenium import webdriver\n",
    "\n",
    "# Packages for cleaning Data\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Packages for PostgreSQL Import\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of urls\n",
    "html_core = \"https://www.guru.com/d/freelancers/lc/united-states/california/los-angeles/pg/\"\n",
    "pg_nums = list(map(str,list(range(1,945))))\n",
    "tmp = [s + \"/\" for s in pg_nums]\n",
    "htmls = [html_core + s for s in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_extract(x):\n",
    "    \"\"\"\n",
    "    Extracts and cleans the html from a website\n",
    "    \n",
    "    Returns soup object from beautiful soup\n",
    "    \"\"\"\n",
    "    source =  requests.get(x).text\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freelancer_extraction(x):\n",
    "    \"\"\"\n",
    "    Extracts freelancer information from a webpage.\n",
    "    \n",
    "    x is a parsed soup object\n",
    "    \n",
    "    Returns a list of freelancers. Requires further cleaning.\n",
    "    See header_content_extraction for next steps.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # extracts the section that contains the freelancer data\n",
    "    a = x.body.form.main.main.section.find_all('ul')[1] \n",
    "\n",
    "    # Create a list where each element is a freelancer.\n",
    "    # The element has a number of different tags for each data point.\n",
    "    b = a.find_all('div',class_ = \"record__details\")\n",
    "    \n",
    "    return b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_content_extraction(x):\n",
    "    \"\"\"\n",
    "    Extract header and content tags from a given freelancer.\n",
    "    \n",
    "    x is a single freelancers HTML information\n",
    "    \n",
    "    Returns a header and content object.\n",
    "        header: contains un-processed information on name, url, location\n",
    "        content: contains un-processed information on skills, rate, description\n",
    "    \n",
    "    \"\"\"\n",
    "    header = x.div\n",
    "    content = x.div.next_sibling.next_sibling\n",
    "    \n",
    "    return header, content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def header_data_extract(x):\n",
    "    \"\"\"\n",
    "    Extract data from header.\n",
    "    \n",
    "    x is the header for a given freelancer.\n",
    "    \n",
    "    Data being extracted:\n",
    "        - profile url\n",
    "        - city\n",
    "        - state\n",
    "        - country\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting data\n",
    "    profile_url = \"https://www.guru.com\" + x.a['href']\n",
    "    \n",
    "    city = x.find('span',class_=\"freelancerAvatar__location--city\").string\n",
    "    state = x.find('span',class_=\"freelancerAvatar__location--state\").string\n",
    "    country = x.find('span',class_=\"freelancerAvatar__location--country\").string\n",
    "    \n",
    "    # Clearning commas off city and state\n",
    "    city = city.replace(',','')\n",
    "    state = state.replace(',','')\n",
    "    \n",
    "    header = {\"profile_url\": profile_url, \"city\": city,\n",
    "              \"state\": state, \"country\": country}\n",
    "    \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_data_extract(x):\n",
    "    \"\"\"\n",
    "    Extracting data from the content\n",
    "    x is the content tag for a given freelancer\n",
    "    \n",
    "    Data being extracted:\n",
    "        - Rates\n",
    "        - Shortened user description (need to go into their profile for detail)\n",
    "        - Skills list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    rates = x.find_all('p')[0].string\n",
    "    user_description_short = x.find_all('p')[1].string\n",
    "    skills_list = x.find_all('a', class_=\"skillsList__skill skillsList__skill--hasHover\")\n",
    "                                   \n",
    "    # Cleaning skills_list from messy html list to list of clean strings\n",
    "    string_clean = lambda skills_list: skills_list.string\n",
    "    skills_list_strings = list(map(string_clean, skills_list))\n",
    "                                   \n",
    "    # Cleaning rates\n",
    "    p = re.compile(r'\\d+')\n",
    "    result = p.findall(rates)\n",
    "    hourly_rate = int(result[0]) # First number is always hourly rate\n",
    "    \n",
    "    # Cleaning user description of indents and return\n",
    "    user_description_short = user_description_short.replace('\\t','')\n",
    "    user_description_short = user_description_short.replace('\\r','')\n",
    "    user_description_short = user_description_short.replace('\\n','')\n",
    "    \n",
    "    # Creating Dictionary\n",
    "    content = {\"hourly_rate\": hourly_rate, \"skills_list\": skills_list_strings,\n",
    "               \"user_description\": user_description_short}\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details_scrape(url):\n",
    "    \"\"\"\n",
    "    Uses selenium to scrape additional information contained in a sub-menu\n",
    "    \n",
    "    Clicks the first box, guru throws up a log-in, goes back, opens all the boxes\n",
    "    and then finishes by extracting all the html from each box.\n",
    "    \n",
    "    This takes in a single URL (string) and extracts from that. One problem that may arise\n",
    "    is if it doesn't throw the log-in box and the driver tries to go back a page.\n",
    "    However, I think we can get around this by just loading each URL new. \n",
    "    \n",
    "    May be more efficient to page through? That's for another time.\n",
    "    \n",
    "    RETURNS: list of names and list of HTML for each details box.\n",
    "    \n",
    "    Test URL:\n",
    "    \"https://www.guru.com/d/freelancers/lc/united-states/california/los-angeles/pg/1/\"\n",
    "    \"\"\"\n",
    "    \n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(url)\n",
    "    login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]')\n",
    "\n",
    "    time.sleep(2) # Wait to click\n",
    "    login_form[2].click()\n",
    "\n",
    "    time.sleep(2) # Wait for log-in prompt\n",
    "    driver.back() # Go back to main page\n",
    "    time.sleep(2) # Wait to scrape\n",
    "\n",
    "    # Pulling the paths again (they change for some reason)\n",
    "    login_form = driver.find_elements_by_xpath('//button[@class=\"tabControls__button\"]') \n",
    "    counter = 2\n",
    "    \n",
    "    # There are multiple buttons so here I am just clicking the ones I want (2,6,10,...)\n",
    "    for i, val in enumerate(login_form):    \n",
    "        if i == counter:\n",
    "            login_form[i].click()\n",
    "            counter += 4\n",
    "    \n",
    "    print(\"Completed opening all the tabs\")\n",
    "\n",
    "    # Pull elements in the now opened detail boxes\n",
    "    user_name = driver.find_elements_by_xpath('//h3[@class=\"freelancerAvatar__screenName\"]')\n",
    "    user_detail = driver.find_elements_by_xpath('//ul[@class=\"feedback__stats clearfix\"]')\n",
    "\n",
    "    # Extract text from the names and HTML from the details\n",
    "    # Will parse the detail_html further with beautiful soup\n",
    "    names = []\n",
    "    for i, val in enumerate(user_name):\n",
    "        names.append(val.text)\n",
    "\n",
    "    detail_html = []\n",
    "    for i, val in enumerate(user_detail):\n",
    "        detail_html.append(val.get_attribute('innerHTML'))\n",
    "\n",
    "    driver.close()\n",
    "    print(\"Finished\")\n",
    "    \n",
    "    return names, detail_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# Putting it all together\n",
    "df = pd.DataFrame(columns = [\"profile_url\",\"city\",\"state\",\"country\",\"hourly_rate\",\"skills_list\",\"user_description\"])\n",
    "\n",
    "for k, page in enumerate(htmls[0:50]): # Make it random sample if you want (print(random.sample(htmls,2)))\n",
    "    \n",
    "    if k%20 == 0:\n",
    "        print(k)\n",
    "\n",
    "    time.sleep(np.random.randint(1,5))\n",
    "        \n",
    "    soup = html_extract(page)\n",
    "    freelancers = freelancer_extraction(soup) # Clean HTML\n",
    "\n",
    "    for k, value in enumerate(freelancers):\n",
    "\n",
    "        header_content = header_content_extraction(value) # Extract two boxes of interest\n",
    "\n",
    "        results = header_data_extract(header_content[0]) # Extract header data\n",
    "        content = content_data_extract(header_content[1]) # Extract content data\n",
    "        results.update(content) # Combine into one dictionary\n",
    "        results = pd.DataFrame(results)\n",
    "        df = df.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'freelance_db'\n",
    "username = 'Metaverse'\n",
    "pswd = 'arcifice91'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://Metaverse:arcifice91@localhost/freelance_db\n",
      "postgresql://Metaverse:arcifice91@localhost/freelance_db\n"
     ]
    }
   ],
   "source": [
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print(engine.url)\n",
    "# Replace localhost with IP address if accessing a remote server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "postgresql://Metaverse:arcifice91@localhost/freelance_db\n"
     ]
    }
   ],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert data into database from Python (proof of concept - this won't be useful for big data, of course)\n",
    "## df is any pandas dataframe \n",
    "df.to_sql('freelance_db', engine, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
