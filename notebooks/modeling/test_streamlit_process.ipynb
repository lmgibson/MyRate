{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Streamlit Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st  # creating web-app\n",
    "import pandas as pd  # managing data\n",
    "import numpy as np  # managing data for model\n",
    "import pickle  # Importing serialized model for prediction\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords  # Cleaning text data\n",
    "import os\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input_cols():\n",
    "    cols = ['num_skills', 'bio_length', 'bio_word_count', 'avg_word_length', 'num_stop',\n",
    "            'administrative & secretarial', 'business & finance', 'design & art', 'education & training',\n",
    "            'engineering & architecture', 'legal', 'programming & development', 'sales & marketing',\n",
    "            'writing & translation', 'Alabama', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "            'Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n",
    "            'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
    "            'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico',\n",
    "            'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Puerto Rico',\n",
    "            'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',\n",
    "            'Washington', 'West Virginia', 'Wisconsin']\n",
    "\n",
    "    col_dict = dict.fromkeys(cols, 0)\n",
    "\n",
    "    return col_dict\n",
    "\n",
    "\n",
    "# Functions to clean bio data\n",
    "\n",
    "\n",
    "def clean_bio(bio):\n",
    "    try:\n",
    "        cleaned_bio = ''.join(s for s in bio if ord(s) > 31 and ord(s) < 126)\n",
    "    except:\n",
    "        cleaned_bio = \"NaN\"\n",
    "\n",
    "    return cleaned_bio\n",
    "\n",
    "\n",
    "def avg_word_ln(bio):\n",
    "    try:\n",
    "        words = bio.split()\n",
    "        res = (sum(len(word) for word in words) / len(words))\n",
    "    except:\n",
    "        res = 0\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def num_stopwords(bio):\n",
    "    stop = stopwords.words('english')\n",
    "\n",
    "    try:\n",
    "        res = len([x for x in bio.split() if x in stop])\n",
    "    except:\n",
    "        res = -1\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# Function to upload model\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # Tries the location for the hosted server first. Then tries local.\n",
    "    model_xgboost = xgb.Booster({'nthread': 4})  # init model\n",
    "\n",
    "    try:\n",
    "        filename = '/home/ubuntu/MyRate/scripts/model_xgb.sav'\n",
    "        model_xgboost = pickle.load(open(filename, 'rb'))\n",
    "    except:\n",
    "        filename = '/Users/Metaverse/Desktop/Insight/projects/myrate/scripts/model_xgb.sav'\n",
    "        model_xgboost = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    return model_xgboost\n",
    "\n",
    "# Functions to Create Word2Vec Embedding from example bio\n",
    "# This creates embedding values for a single user.\n",
    "# It works by looping over every word, estimating it's value, and adding it to the feature vector\n",
    "# For words not in the vocabulary it will return a vector of zeros.\n",
    "# After this process we get a matrix that is n_words x 50.\n",
    "# Finally, we collapse this array along the rows by taking the mean to obtain a single\n",
    "# 1x50 embedding vector that can be fed into the model.\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                for tokenized_sentence in corpus]\n",
    "\n",
    "    return np.mean(pd.DataFrame(np.array(features)), axis=0)\n",
    "\n",
    "\n",
    "def get_word_embedding():\n",
    "    # Loading Model\n",
    "    filename = os.environ['PWD'] + '/scripts/model_w2v.sav'\n",
    "    model_w2v = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    tokenized_corpus = word_tokenize(bio)\n",
    "    embeddings = averaged_word_vectorizer(corpus=tokenized_corpus, model=model_w2v,\n",
    "                                          num_features=50)\n",
    "    \n",
    "    embeddings.index = [str(x) for x in list(embeddings.index)]\n",
    "    embeddings = pd.DataFrame(embeddings).iloc[:, 0].to_dict()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Code to estimate hourly rate\n",
    "def create_input_array():\n",
    "    # Updating Bio Related Features\n",
    "    cols['num_skills'] = 5\n",
    "    cols['bio_length'] = bio_length\n",
    "    cols['bio_word_count'] = bio_word_count\n",
    "    cols['avg_word_length'] = bio_avg_word_length\n",
    "    cols['num_stop'] = bio_num_stop\n",
    "    cols.update(w2v_embedding)\n",
    "\n",
    "    # Updating Location Feature\n",
    "    cols[state] = 1\n",
    "    for i, val in enumerate(skill_categories):\n",
    "        cols[val.lower()] = 1\n",
    "        \n",
    "    return pd.DataFrame(cols, index=[0])\n",
    "\n",
    "def estimate_hourly_rate():\n",
    "    # Predicting\n",
    "    pred = model.predict(xgb.DMatrix(cols.values))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Metaverse/Desktop/Insight/projects/myrate/conda-env/lib/python3.7/site-packages/ipykernel_launcher.py:83: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "bio = \"Scopic is a large web development team\"\n",
    "bio_clean = clean_bio(bio)\n",
    "bio_length = len(bio_clean)\n",
    "bio_avg_word_length = avg_word_ln(bio_clean)\n",
    "bio_num_stop = num_stopwords(bio_clean)\n",
    "bio_word_count = len(str(bio_clean).split(\" \"))\n",
    "w2v_embedding = get_word_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = model_input_cols()\n",
    "state = 'California'\n",
    "skill_categories = ['programming & development']\n",
    "cols = create_input_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_skills</th>\n",
       "      <th>bio_length</th>\n",
       "      <th>bio_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>num_stop</th>\n",
       "      <th>administrative &amp; secretarial</th>\n",
       "      <th>business &amp; finance</th>\n",
       "      <th>design &amp; art</th>\n",
       "      <th>education &amp; training</th>\n",
       "      <th>engineering &amp; architecture</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068555</td>\n",
       "      <td>0.039563</td>\n",
       "      <td>0.031338</td>\n",
       "      <td>-0.037409</td>\n",
       "      <td>-0.014804</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.044714</td>\n",
       "      <td>0.05703</td>\n",
       "      <td>-0.202112</td>\n",
       "      <td>-0.138371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_skills  bio_length  bio_word_count  avg_word_length  num_stop  \\\n",
       "0           5          38               7         4.571429         2   \n",
       "\n",
       "   administrative & secretarial  business & finance  design & art  \\\n",
       "0                             0                   0             0   \n",
       "\n",
       "   education & training  engineering & architecture  ...        40        41  \\\n",
       "0                     0                           0  ...  0.068555  0.039563   \n",
       "\n",
       "         42        43        44        45        46       47        48  \\\n",
       "0  0.031338 -0.037409 -0.014804  0.000015 -0.044714  0.05703 -0.202112   \n",
       "\n",
       "         49  \n",
       "0 -0.138371  \n",
       "\n",
       "[1 rows x 113 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.137676], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "estimate_hourly_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
