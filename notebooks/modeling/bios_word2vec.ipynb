{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Analysis of User Bios\n",
    "References:\n",
    "    - http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    - https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    - https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for PostgreSQL Import and Export\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Data Management and Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Word2Vec\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "\n",
    "# Further Reducing the Dimensionality of Word Embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# NLTK for working with text data (cleaning and processing)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Other\n",
    "from itertools import chain\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"freelance_db\"\n",
    "username = os.environ['USER']\n",
    "pswd = os.environ['SQLPSWD']\n",
    "\n",
    "# Connect to Database\n",
    "con = None\n",
    "con = psycopg2.connect(database=dbname, user=username,\n",
    "                       host='localhost', password=pswd)\n",
    "\n",
    "# Extracting table with user profile and bio\n",
    "sql_query = \"\"\"SELECT profile_url, bio from user_details_table;\"\"\"\n",
    "bio_table = pd.read_sql_query(sql_query, con)\n",
    "bio_table = bio_table.dropna() # Removing users with no bio\n",
    "bio_table = bio_table.loc[bio_table.bio != \"NA\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining a Word2Vec Embedding\n",
    "\n",
    "The idea here is to take the word vector I previously built and reduce it's dimensionality. Then I can insert that into the model as a new feature. I made a realization that I don't have to have it by users. I can simply put all the sentences together. The point is to train a model.\n",
    "\n",
    "Reference: https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentences from bios\n",
    "data = bio_table.bio.tolist()\n",
    "sentences = [sent_tokenize(text) for text in data]\n",
    "sentences = list(chain.from_iterable(sentences))\n",
    "\n",
    "# Lowercasing each sentence\n",
    "sentences = [w.lower() for w in sentences]\n",
    "\n",
    "# Splitting sentences into words\n",
    "words = [s.split(' ') for s in sentences]\n",
    "\n",
    "# Removing punctuation from each word\n",
    "words = [[w.translate(str.maketrans('', '', string.punctuation)) for w in x] for x in words]\n",
    "\n",
    "# Removing stop words\n",
    "stoplist = stopwords.words('english')\n",
    "words = [[x for x in s if x not in stoplist] for s in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec\n",
    "size = 50 # Don't go higher than 10% 100 or 200 will be sufficient; Check overfitting drop between train and test.\n",
    "model = gensim.models.Word2Vec(words, min_count=5, size = size, workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('developer', 0.9998610615730286),\n",
       " ('services', 0.9998580813407898),\n",
       " ('application', 0.9998360872268677),\n",
       " ('development', 0.9998316168785095),\n",
       " ('including', 0.9998306035995483)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['web'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Model with pickle\n",
    "filename = os.environ['PWD'] + '/scripts/models/model_w2v.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentences from bios\n",
    "data = bio_table.bio.tolist()\n",
    "\n",
    "tokenized_corpus = [word_tokenize(s) for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extracts the embeddings at the sentence level and then averages across sentences within\n",
    "# a document in order to obtain document (user) level embedding vector\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Metaverse/Desktop/Insight/projects/myrate/conda-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Extracting Datafrom \n",
    "words = model.wv.index2word\n",
    "wvs = model.wv[words]\n",
    "\n",
    "# get document level embeddings\n",
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=model,\n",
    "                                             num_features=size)\n",
    "embeddings = pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Embeddings to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>profile_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.473687</td>\n",
       "      <td>-0.083848</td>\n",
       "      <td>0.146523</td>\n",
       "      <td>-0.094585</td>\n",
       "      <td>-0.364188</td>\n",
       "      <td>0.158576</td>\n",
       "      <td>-0.071422</td>\n",
       "      <td>-0.367279</td>\n",
       "      <td>-0.087792</td>\n",
       "      <td>0.526227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423684</td>\n",
       "      <td>0.511746</td>\n",
       "      <td>-0.046579</td>\n",
       "      <td>-0.475470</td>\n",
       "      <td>-0.171319</td>\n",
       "      <td>-0.202593</td>\n",
       "      <td>0.092216</td>\n",
       "      <td>-0.391020</td>\n",
       "      <td>-0.543387</td>\n",
       "      <td>https://www.guru.com/freelancers/scopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.397739</td>\n",
       "      <td>-0.072081</td>\n",
       "      <td>0.125606</td>\n",
       "      <td>-0.082030</td>\n",
       "      <td>-0.310165</td>\n",
       "      <td>0.133467</td>\n",
       "      <td>-0.059300</td>\n",
       "      <td>-0.313602</td>\n",
       "      <td>-0.073510</td>\n",
       "      <td>0.443927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359313</td>\n",
       "      <td>0.432702</td>\n",
       "      <td>-0.040519</td>\n",
       "      <td>-0.403511</td>\n",
       "      <td>-0.146243</td>\n",
       "      <td>-0.171523</td>\n",
       "      <td>0.074763</td>\n",
       "      <td>-0.329972</td>\n",
       "      <td>-0.459600</td>\n",
       "      <td>https://www.guru.com/freelancers/top-guru-assi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.536638</td>\n",
       "      <td>-0.097036</td>\n",
       "      <td>0.173734</td>\n",
       "      <td>-0.108803</td>\n",
       "      <td>-0.413958</td>\n",
       "      <td>0.174294</td>\n",
       "      <td>-0.078517</td>\n",
       "      <td>-0.427603</td>\n",
       "      <td>-0.097882</td>\n",
       "      <td>0.596463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478170</td>\n",
       "      <td>0.583258</td>\n",
       "      <td>-0.058174</td>\n",
       "      <td>-0.539831</td>\n",
       "      <td>-0.194414</td>\n",
       "      <td>-0.231520</td>\n",
       "      <td>0.101226</td>\n",
       "      <td>-0.439445</td>\n",
       "      <td>-0.621933</td>\n",
       "      <td>https://www.guru.com/freelancers/eden-programm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.467405</td>\n",
       "      <td>-0.085745</td>\n",
       "      <td>0.147209</td>\n",
       "      <td>-0.094812</td>\n",
       "      <td>-0.362905</td>\n",
       "      <td>0.156062</td>\n",
       "      <td>-0.068085</td>\n",
       "      <td>-0.366998</td>\n",
       "      <td>-0.087990</td>\n",
       "      <td>0.517890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418161</td>\n",
       "      <td>0.507364</td>\n",
       "      <td>-0.048871</td>\n",
       "      <td>-0.471296</td>\n",
       "      <td>-0.169405</td>\n",
       "      <td>-0.197554</td>\n",
       "      <td>0.089430</td>\n",
       "      <td>-0.383217</td>\n",
       "      <td>-0.538900</td>\n",
       "      <td>https://www.guru.com/freelancers/avion-technol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.458545</td>\n",
       "      <td>-0.082751</td>\n",
       "      <td>0.145163</td>\n",
       "      <td>-0.093410</td>\n",
       "      <td>-0.351569</td>\n",
       "      <td>0.151399</td>\n",
       "      <td>-0.066140</td>\n",
       "      <td>-0.360898</td>\n",
       "      <td>-0.086942</td>\n",
       "      <td>0.508418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409719</td>\n",
       "      <td>0.498466</td>\n",
       "      <td>-0.046255</td>\n",
       "      <td>-0.460647</td>\n",
       "      <td>-0.165392</td>\n",
       "      <td>-0.194842</td>\n",
       "      <td>0.087352</td>\n",
       "      <td>-0.374891</td>\n",
       "      <td>-0.527440</td>\n",
       "      <td>https://www.guru.com/freelancers/j-consulting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>-0.437466</td>\n",
       "      <td>-0.078369</td>\n",
       "      <td>0.139276</td>\n",
       "      <td>-0.088720</td>\n",
       "      <td>-0.335110</td>\n",
       "      <td>0.143663</td>\n",
       "      <td>-0.068917</td>\n",
       "      <td>-0.344429</td>\n",
       "      <td>-0.080906</td>\n",
       "      <td>0.480431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394473</td>\n",
       "      <td>0.469618</td>\n",
       "      <td>-0.043726</td>\n",
       "      <td>-0.436772</td>\n",
       "      <td>-0.159378</td>\n",
       "      <td>-0.186742</td>\n",
       "      <td>0.087153</td>\n",
       "      <td>-0.357306</td>\n",
       "      <td>-0.498007</td>\n",
       "      <td>https://www.guru.com/freelancers/leslie-mestrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>-0.389108</td>\n",
       "      <td>-0.071778</td>\n",
       "      <td>0.123694</td>\n",
       "      <td>-0.079138</td>\n",
       "      <td>-0.302635</td>\n",
       "      <td>0.129613</td>\n",
       "      <td>-0.059151</td>\n",
       "      <td>-0.306527</td>\n",
       "      <td>-0.072863</td>\n",
       "      <td>0.431356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351334</td>\n",
       "      <td>0.422329</td>\n",
       "      <td>-0.038835</td>\n",
       "      <td>-0.393794</td>\n",
       "      <td>-0.142296</td>\n",
       "      <td>-0.165632</td>\n",
       "      <td>0.073958</td>\n",
       "      <td>-0.320322</td>\n",
       "      <td>-0.448946</td>\n",
       "      <td>https://www.guru.com/freelancers/four-winds-gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>-0.263986</td>\n",
       "      <td>-0.051583</td>\n",
       "      <td>0.084650</td>\n",
       "      <td>-0.051157</td>\n",
       "      <td>-0.205489</td>\n",
       "      <td>0.086467</td>\n",
       "      <td>-0.041054</td>\n",
       "      <td>-0.205224</td>\n",
       "      <td>-0.049693</td>\n",
       "      <td>0.294936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239662</td>\n",
       "      <td>0.286001</td>\n",
       "      <td>-0.029092</td>\n",
       "      <td>-0.267428</td>\n",
       "      <td>-0.098488</td>\n",
       "      <td>-0.115194</td>\n",
       "      <td>0.049589</td>\n",
       "      <td>-0.217621</td>\n",
       "      <td>-0.305413</td>\n",
       "      <td>https://www.guru.com/freelancers/robert-bennear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>-0.514803</td>\n",
       "      <td>-0.094894</td>\n",
       "      <td>0.162922</td>\n",
       "      <td>-0.104494</td>\n",
       "      <td>-0.399490</td>\n",
       "      <td>0.170641</td>\n",
       "      <td>-0.077902</td>\n",
       "      <td>-0.401780</td>\n",
       "      <td>-0.095346</td>\n",
       "      <td>0.570083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460974</td>\n",
       "      <td>0.557570</td>\n",
       "      <td>-0.052579</td>\n",
       "      <td>-0.517442</td>\n",
       "      <td>-0.188584</td>\n",
       "      <td>-0.218128</td>\n",
       "      <td>0.098148</td>\n",
       "      <td>-0.418702</td>\n",
       "      <td>-0.588495</td>\n",
       "      <td>https://www.guru.com/freelancers/david-cas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>-0.349243</td>\n",
       "      <td>-0.064923</td>\n",
       "      <td>0.111818</td>\n",
       "      <td>-0.071198</td>\n",
       "      <td>-0.272606</td>\n",
       "      <td>0.113858</td>\n",
       "      <td>-0.055138</td>\n",
       "      <td>-0.273183</td>\n",
       "      <td>-0.065323</td>\n",
       "      <td>0.387554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314603</td>\n",
       "      <td>0.378192</td>\n",
       "      <td>-0.034759</td>\n",
       "      <td>-0.353251</td>\n",
       "      <td>-0.128939</td>\n",
       "      <td>-0.150104</td>\n",
       "      <td>0.066028</td>\n",
       "      <td>-0.285293</td>\n",
       "      <td>-0.400903</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>785 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.473687 -0.083848  0.146523 -0.094585 -0.364188  0.158576 -0.071422   \n",
       "1   -0.397739 -0.072081  0.125606 -0.082030 -0.310165  0.133467 -0.059300   \n",
       "2   -0.536638 -0.097036  0.173734 -0.108803 -0.413958  0.174294 -0.078517   \n",
       "3   -0.467405 -0.085745  0.147209 -0.094812 -0.362905  0.156062 -0.068085   \n",
       "4   -0.458545 -0.082751  0.145163 -0.093410 -0.351569  0.151399 -0.066140   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "780 -0.437466 -0.078369  0.139276 -0.088720 -0.335110  0.143663 -0.068917   \n",
       "781 -0.389108 -0.071778  0.123694 -0.079138 -0.302635  0.129613 -0.059151   \n",
       "782 -0.263986 -0.051583  0.084650 -0.051157 -0.205489  0.086467 -0.041054   \n",
       "783 -0.514803 -0.094894  0.162922 -0.104494 -0.399490  0.170641 -0.077902   \n",
       "784 -0.349243 -0.064923  0.111818 -0.071198 -0.272606  0.113858 -0.055138   \n",
       "\n",
       "            7         8         9  ...        41        42        43  \\\n",
       "0   -0.367279 -0.087792  0.526227  ...  0.423684  0.511746 -0.046579   \n",
       "1   -0.313602 -0.073510  0.443927  ...  0.359313  0.432702 -0.040519   \n",
       "2   -0.427603 -0.097882  0.596463  ...  0.478170  0.583258 -0.058174   \n",
       "3   -0.366998 -0.087990  0.517890  ...  0.418161  0.507364 -0.048871   \n",
       "4   -0.360898 -0.086942  0.508418  ...  0.409719  0.498466 -0.046255   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "780 -0.344429 -0.080906  0.480431  ...  0.394473  0.469618 -0.043726   \n",
       "781 -0.306527 -0.072863  0.431356  ...  0.351334  0.422329 -0.038835   \n",
       "782 -0.205224 -0.049693  0.294936  ...  0.239662  0.286001 -0.029092   \n",
       "783 -0.401780 -0.095346  0.570083  ...  0.460974  0.557570 -0.052579   \n",
       "784 -0.273183 -0.065323  0.387554  ...  0.314603  0.378192 -0.034759   \n",
       "\n",
       "           44        45        46        47        48        49  \\\n",
       "0   -0.475470 -0.171319 -0.202593  0.092216 -0.391020 -0.543387   \n",
       "1   -0.403511 -0.146243 -0.171523  0.074763 -0.329972 -0.459600   \n",
       "2   -0.539831 -0.194414 -0.231520  0.101226 -0.439445 -0.621933   \n",
       "3   -0.471296 -0.169405 -0.197554  0.089430 -0.383217 -0.538900   \n",
       "4   -0.460647 -0.165392 -0.194842  0.087352 -0.374891 -0.527440   \n",
       "..        ...       ...       ...       ...       ...       ...   \n",
       "780 -0.436772 -0.159378 -0.186742  0.087153 -0.357306 -0.498007   \n",
       "781 -0.393794 -0.142296 -0.165632  0.073958 -0.320322 -0.448946   \n",
       "782 -0.267428 -0.098488 -0.115194  0.049589 -0.217621 -0.305413   \n",
       "783 -0.517442 -0.188584 -0.218128  0.098148 -0.418702 -0.588495   \n",
       "784 -0.353251 -0.128939 -0.150104  0.066028 -0.285293 -0.400903   \n",
       "\n",
       "                                           profile_url  \n",
       "0              https://www.guru.com/freelancers/scopic  \n",
       "1    https://www.guru.com/freelancers/top-guru-assi...  \n",
       "2    https://www.guru.com/freelancers/eden-programm...  \n",
       "3    https://www.guru.com/freelancers/avion-technol...  \n",
       "4        https://www.guru.com/freelancers/j-consulting  \n",
       "..                                                 ...  \n",
       "780    https://www.guru.com/freelancers/leslie-mestrow  \n",
       "781  https://www.guru.com/freelancers/four-winds-gr...  \n",
       "782    https://www.guru.com/freelancers/robert-bennear  \n",
       "783         https://www.guru.com/freelancers/david-cas  \n",
       "784                                                NaN  \n",
       "\n",
       "[785 rows x 51 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding profile_urls for merging later\n",
    "embeddings['profile_url'] = \"https://www.guru.com\" + bio_table.profile_url\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding profile_urls for merging later\n",
    "embeddings['profile_url'] = \"https://www.guru.com\" + bio_table.profile_url\n",
    "\n",
    "# Connect to the database and save data to it\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s' %\n",
    "                       (username, pswd, dbname))\n",
    "embeddings.to_sql(\"embeddings_table\", engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = bio_table.bio.tolist()\n",
    "\n",
    "# Create a set of frequent words\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "# Tokenize each document\n",
    "texts = [text.lower().split() for text in text_corpus]\n",
    "\n",
    "# Removing punctuation\n",
    "texts = [[s.translate(str.maketrans('', '', string.punctuation)) for s in word] for word in texts]\n",
    "\n",
    "# Removing stop words\n",
    "texts = [[x for x in words if x not in stoplist] for words in texts]\n",
    "\n",
    "# Count word frequencies\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [\n",
    "    [token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "# Creating Word Dictionary\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "num_features = len(dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert entire corpus to vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Create similarity index\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features = num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing against a new example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing new example\n",
    "query_document = 'human interaction and engineering data c# analysis'.split()\n",
    "\n",
    "# Converting to vector\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "\n",
    "# Checking similarity of tfidif conversion of vector\n",
    "sims = index[tfidf[query_bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing similarity scores\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction on Embeddings\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting PCA Model to aggregated embeddings (or should I do this to the fully embedding then aggregate?)\n",
    "x = StandardScaler().fit_transform(w2v_feature_array)\n",
    "pca = PCA(.95)\n",
    "\n",
    "pca.fit(x)\n",
    "out = pd.DataFrame(pca.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.singular_values_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
